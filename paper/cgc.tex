\section{Composable Generalized Consensus}

We consider a computing service specified as a deterministic state machine $M$ whose transitions correspond to the commands that clients of the service may issue.
If $M$ is implemented on a single server then any fault happening on this server may become visible to the clients of the service.
To mask faults, one can replicate the implementation of $M$ on several replica servers (called replicas for short) and direct clients' commands to replicas which are known to be non-faulty.
However, the intent of replication is to mask faults while preserving the behavior of the service from the point of view of its clients.
Therefore, replicas must be somehow synchronized to act as a centralized implementation.

State-Machine Replication~\cite{Schneider90ImplementingFaulttolerantServicesUsingStateMachine} aims at orchestrating the execution of the replicas to ensure that clients have the illusion of accessing a unique and centralized implementation of $M$ despite a number of faults, which must remain below a certain maximum depending on the algorithms used.
State-Machine Replication consists in replicating an implementation of $M$ over the replicas and ensuring with a distributed algorithm $A$ that all the replica execute the same sequence of commands, possibly up to the reordering of commuting commands.

\subsection{Generalized Consensus}

Composable Generalized Consensus is based on Generalized Consensus, and any CGC algorithm can be used as a Generalized Consensus algorithm. Therefore, we start by a review of generalized and present CGC in the next subsection.

Generalized Consensus~\cite{Lamport05GeneralizeConsensus} is a formal specification of the requirements that the distributed algorithm $A$ described above must meet in order for SMR to provide the illusion of a centralized fault-tolerant service.
Generalized Consensus uses the notion of command-structures, or c-structs. C-structs are obtained by appending commands to the c-struct $\bot$. Given a c-struct $s$ and a command $c$, $s \bullet c$ is the c-struct obtained by appending $c$ to $s$. Given a sequence of commands $cs$, $s\star cs$ is the c-struct obtained by repeatedly appending the commands in $cs$ in the order where they appear in $cs$. By appropriately defining the operator $\bullet$ according to the
definition of the state-machine $M$, c-structs can stand for sequences of commands up the reordering of commuting
commands; for example, we can have $s \star \left[c_1,c_2\right] = s \star \left[c_2,c_1\right]$ for some c-struct $s$ and two commands $c_1$ and $c_2$.

C-structs must satisfy a number of properties: \setword{property 1}{prop:1}, c-structs must form a partial order under the operator $\preceq$ defined as: $s_1 \preceq s_2$ if and only if there exists a sequence of commands $cs$ such that $s_2 = s_1 \star cs$; \setword{property 2}{prop:2}, the greatest lower bound of a set of c-structs $S$ must always exist and be constructible from the same commands as the c-structs in $S$; and \setword{property 3}{prop:3}, a set of c-structs $S$
which has an upper bound, called a \emph{compatible} set, must have a least upper bound which is constructible from the same commands as the c-structs in $S$. 

Generalized Consensus is formulated in terms of two types of processes: the set $P$ of proposers and the set $L$ of learners, but it is possible for each replica to play the two roles at the same time. Proposers propose commands and learners, which represent replicas, learn about c-structs.

We specify the execution of learners and proposers in terms of a transition system consisting of the variable $prop$, which is a set containing all the commands that have been proposed, and the variable $learned$, which maps a learner to the latest c-structs it has learned.

Generalized Consensus requires the following properties: \emph{non-triviality}, stating that for any learner $l$, there is at all times a sequence of proposed commands $cs$ such that $learned\left[l\right] = \bot \bullet cs$; \setword{\emph{stability}}{prop:stability}, stating that for any learner $l$, $learned\left[ l \right]$ increase with time; \emph{agreement}, stating that $\left\{ learned\left[ l \right] :  l \in L\right\}$ is always \emph{compatible}; and \emph{liveness}, stating that for any proposed command $c$ and learner $l$, a c-struct of the form $s\bullet c$ is eventually learned by $l$.
Note that the FLP theorem~\cite{FischerLynchPaterson83ImpossibilityDistributedConsensusOneFaultyProcess} implies that the liveness property cannot be achieved in a networked system where processes may fail. Therefore, the liveness property guaranteed by Generalized Consensus algorithms is necessary weaker that the liveness property state above, maybe adding assumptions about faults and synchrony.
We refer the reader to the work of Lamport~\cite{Lamport05GeneralizeConsensus} for a thorough discussion of Generalized Consensus.

\subsection{Composable Generalized Consensus}

Composable Generalized Consensus augments the Generalized Consensus specification with an abort interface that allows an SMR module to abort its execution, and with an init interface which allows an SMR module to start its execution where a preceding aborted module left off.
To specify the composition interface, we introduce a third type of processes called the \emph{switchers}. Switchers can receive \emph{init c-structs} from a preceding aborting module and can pass \emph{abort c-structs} to a next SMR module, which will treat them as init c-structs, when the current SMR module aborts.

We describe the behavior of switchers using the variable $from$, which contains all the init c-structs received by the switchers, and the variable $to$, which contains all the abort c-structs produced by the switchers.
The variable $prop$ is used as before, but the variable $learned$ now maps learners to sets of c-structs and, for each learner $l$, $learned\left[ l \right]$ consists of all the c-structs learned so far by $l$.
We define the set of valid c-structs as the c-structs of the form $t\star cs$ where $cs$ is a sequence of proposed commands, and $t$ is the greatest lower bound of a non-empty subset of the init c-structs received.

Composable Generalized Consensus requires the following properties: \emph{initialization}, stating that no value be learned unless at least one init c-struct has been received; \emph{non-triviality}, stating that for any learner $l$, $learned\left[ l \right]$ is valid; \emph{agreement}, stating that $\left\{ learned\left[ l \right] :  l \in L\right\}$ is always compatible; \emph{safe-abort}, stating that every abort c-struct is valid, and that for any abort c-struct $s_a$ and learned value $s_l$ we have
$s_l \preceq s_a$; and \emph{liveness}, stating that for any proposed command $c$ and learner $l$, a c-struct of the form $s\bullet c$ is eventually learned by $l$ or a switcher eventually produces an abort c-struct.
A TLA$^+$ specification of the safety properties of CGC appears in \cref{fig:cgc}.

We say that an algorithm implementing CGC under some refinement mapping is a \emph{CGC instance}, and that two CGC instances $i_1$ and $i_2$ are compatible if they update disjoint variables except for the $to$ variable of $i_1$ and the $from$ variable of $i_2$ which must be identical, therefore identifying the abort c-struct of $i_1$ with the init c-structs of $i_2$.
%Note that in the execution of two compatible instances, any abort c-struct produced by the first instance is an init c-struct for the second instance.
We say that a CGC instance is an initial instance when its set of init c-structs is always a subset of $\left\{ \bot \right\}$.   

CGC enjoys two main properties: first, the composition of two CGC instances is itself a CGC instance if the first instance is an initial instance; second, an initial CGC instance satisfies the safety properties of Generalized Consensus under the restrictions explained below.
We call the first property the \emph{composition theorem}.
A formal statement of the composition theorem appears in \cref{fig:compthm}, and an informal proof appears in \cref{sec:compproof}. 
Note that TLA$^+$ is a second order logic and can therefore not express the composition theorem as stated above because we cannot quantify of modules.
Instead, we state the composition theorem for two CGC instances obtained by only renaming the variables of the CGC specification.
Any CGC instance would trivially refine both of the two instances used in the formalization of the composition theorem.
Therefore, we obtain the composition theorem by transitivity and monotonicity of refinement~\cite{AbadiLamport91ExistenceRefinementMappings}.
To show that an initial instance implements Generalized Consensus, we must first wrap it in a small layer that prevents the learned c-struct of a learner from changing non-monotonically. This is because the Generalized Consensus specification requires the learned c-struct of a learner to grow monotonically (the \ref{prop:stability} property), whereas CGC allows the learned c-struct of a learner change non-monotonically. However, this is not a fundamental difference: when a learner
learns a new c-struct in CGC, the additional layer simply updates the learned c-struct of a learner to the least upper-bound of its previous value and the value learned in CCG, thereby guaranteeing the stability property of Generalized Consensus.

\begin{figure}
\begin{minipage}[t]{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth,trim={129pt 397pt 225pt 128pt},clip]{../TLA/ComposableGC.pdf}
    \caption{TLA$⁺$ specification of CGC.}\label{fig:cgc}%
\end{minipage}\hfill
\begin{minipage}[t]{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth,trim={129pt 373pt 220pt 128pt},clip]{../TLA/CompositionTheorem.pdf}
    \caption{The composition theorem in TLA$^⁺$.}\label{fig:compthm}%
\end{minipage}
\end{figure}

\begin{figure}
\begin{minipage}[t]{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth,trim={129pt 455 220pt 128pt},clip]{../TLA/InitialIsGC.pdf}
    \caption{Initial instances implement Generalized Consensus.}\label{fig:cgc}%
\end{minipage}\hfill
\end{figure}

\subsection{Proof of the Composition Theorem}%
\label{sec:compproof}
\begin{comment}
To prove the composition theorem, as expressed in \cref{fig:compthm}, we must show that $Init$ implies $CGC!Init$ and that every transition of $Spec \equiv Init \wedge \box\left[Next\right]_{vars}$ is a transition of $CGC!Spec$.
Below we sketch a proof of the second condition.

First note that the following invariants hold of the execution of $Spec$: \emph{invariant 1}, stating that any learned c-struct of $CGC1$ is smaller than any abort c-struct of $CGC1$; \emph{invariant 2}, stating that for any non-empty subset $S$ of the abort c-structs of $CGC1$, there is a sequence $cs$ of commands proposed in $CGC1$ such that $\operatorname{GLB}\left( S \right) = \bot\star cs$; and \emph{invariant 3}, stating that for every learned c-struct or abort c-struct $s$ of $CGC2$, there is a non-empty subset $S$ of the init c-structs of $CGC2$ and a sequence of commands proposed in $CGC2$ such that $s  = \operatorname{GLB}\left( S \right)\star cs$.
Invariant 1 is enforced by the first conjunct of the $Learn(l)$ transition of $CGC1$, and invariant 3 is enforced by the quantifier bound of the $Learn(l)$ and $Abort$ transition of $CGC2$, using the definition of $Valid$. 
To see why invariant 2 holds, note that the quantifier bound of the $Abort$ transition of $CGC1$, the fact that the only init c-struct of $CGC1$ is $\left\{ \bot \right\}$, and the definition of $Valid$ ensure that any abort c-struct of $CGC1$ is of the form $\bot\star cs$ where $cs$ is a sequence of proposed commands of $CGC1$. Then, we obtain invariant 2 using the~\ref{prop:2} of c-structs.

We now prove that each transition of $Spec$ is a transition (i.e.\ implies a transition) of $CGC!Spec$, analysing the transition one by one.
Note that the transition $CGC1!Propose$ inserts $c$ in the set $CGC!prop$, because $CGC!prop = CGC1!prop \cup CGC2!prop$; therefore, $CGC1!Propose$ implies $CGC!Propose$.
Similarly, the $CGC2!Propose$ implies $CGC!Propose$.
Note that the transition $CGC1!InitM$ inserts $\bot$ in the set $CGC!from$, because $CGC1$ is an initial instance and $CGC!from = CGC1!from$; therefore, $CGC1!InitM$ implies $CGC!InitM$.
The $CGC1!Abort$ transition does not modify any variable of the module instance $CGC$; therefore, $CGC1!Abort$ is a stuttering transition of $CGC$.
Similarly, the $CGC2!InitM$ transition is a stuttering transition of $CGC$. 

Note that the $CGC1!Learn(l)$ transition inserts a c-struct $s$ in the set $CGC!learned\left[ l \right]$, because $CGC!leanred[l] = CGC1!learned\left[ l \right] \cup CGC2!learned\left[ l \right]$; therefore, to show that $CGC1!Learn\left( l \right)$ implies $CGC!Learn\left( l \right)$, we must show (1), that $s$ is smaller than any abort c-struct of $CGC$, (2), that $s$ is compatible with any learned c-struct of $CGC$, and (3), that $s$ is valid in $CGC$. 
To show (1), consider an abort c-struct $a$ of $CGC2$. By invariant 3, $a$ is of the form $\operatorname{GLB}\left( S \right)\star cs$ where $S$ is a subset of the init c-structs of $CGC2$. Therefore, $\operatorname{GLB}\left( S \right)\preceq a$. Moreover, $s$ is smaller than any init c-struct of $CGC2$, by invariant 1. Therefore, $\operatorname{GLB}\left( S \right)$ is bigger than $s$. By transitivity of $\preceq$, we get that $s$ is smaller than $a$.
To show (2), consider a learned c-struct $s_l$ of $CGC$. There are two cases: (2a), $s_l$ is a learned c-struct of $CGC1$, and (2b), $s_l$ is a learned c-struct of $CGC2$. In case (2a), the second conjunct of the $CGC1!Learn(l)$ transition establishes that $s$ and $s_l$ are compatible. In case (2b), by reasoning as in (1), we establish that $s \preceq s_l$. Therefore, $s$ and $s_l$ are compatible.
To show (3), observe that any c-struct valid in $CGC1$ is also valid in $CGC2$.

Similarly, the $CGC2!Learn(l)$ transition inserts a valid c-struct $s$ in the set $CGC!learned\left[ l \right]$, and we must show the same 3 properties.
Property (1) is established by the first conjunct of the $CGC2!Learn\left( l \right)$ transition, using the fact that $CGC!to = CGC2!to$.
To show (2), consider a learned c-struct of $CGC$.
There are two cases: (2a), $s_l$ is a learned c-struct of $CGC1$, and (2b), $s_l$ is a learned c-struct of $CGC2$. In case (2b), the second conjunct of the $CGC2!Learn(l)$ transition establishes that $s$ and $s_l$ are compatible. In case (2a), by reasoning similarly as in TODO use an invariant.
To show (3)\ldots
TODO\@: complete.
We have formalized informal proof above in Isabelle/HOL (and not TLAPS~\cite{CousineauETAL12TlaProofs}, because the authors are proficient in Isabelle/HOL but not in TLAPS); the proof is available at \url{http://losa.fr/cgc}.


\section{Deploying SMR modules in practice}

\subsection{Agreeing on the Next Module}

\end{comment}
