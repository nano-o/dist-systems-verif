\documentclass{llncs}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{comment}
\usepackage{datetime}
\usepackage[url=true,doi=false,isbn=false,backend=biber]{biblatex}
\usepackage{mathtools}
\usepackage{paralist}
\usepackage{mathtools}

%\input{header.tex}
\input{defs.tex}

\addbibresource{managed.bib}

\title{Composable Generalized Consensus}

\author{
  Rachid Guerraoui\\
  \texttt{rachid.guerraoui@epfl.ch}
  \and
  Giuliano Losa\\
  \texttt{giuliano@losa.fr}
}

\date{}

\begin{document}

\maketitle

\input{abstract3}

%\vspace{2cm}
%\begin{center}
%This is a regular paper.
%\end{center}

\newpage

\input{intro}

%\input{roadmap}


\section{Related Work}
\label{sec:related}

Our work is closely related to the Abstract
framework~\cite{GuerraouiETAL10Next700BftProtocols}
and the Speculative Linearizability
framework~\cite{GuerraouiKuncakLosa12SpeculativeLinearizability}. The
Abstract framework pioneered the idea of decomposing SMR algorithms in
independent modules and experimentally showed that the resulting composite
algorithms outperformed state-of-the-art monolithic algorithms. Compared
to our work, the Abstract framework is not precisely formalized and lacks
tools supporting the development of correct algorithms in the framework,
which are direly needed given the notorious complexity of SMR algorithms.
Moreover, the Abstract framework does not allow optimizing for the execution
of commuting commands. However, the Abstract framework supports byzantine
fault-tolerant~\cite{LamportShostakPease82ByzantineGeneralsProblem} SMR
algorithms, which we do not. 

The Speculative Linearizability framework was a
first attempts by the authors to formalize the Abstract framework, provide
formal proof support in Isabelle/HOL~\cite{NipkowPaulsonWenzel02IsabelleHOL},
and extend the Abstract framework to support algorithms that optimize
the execution of commuting commands by requiring Speculatively
Linearizable modules to refine Abstract modules (under a forward/backward
simulation~\cite{LynchVaandrager95ForwardBackwardSimulationsIUntimedSystems})
modulo an equivalence relation on histories of commands that reflects the
commutativity relation among commands. However, the simulation technique led to
complex correctness statements that were hard to scale beyond the examples of
the paper, and no realistic protocol was built in the framework. The present
work addresses the shortcoming of the Speculatively Linearizable framework by
using the notion of command-structure set introduced for the definition of
Generalized Consensus~\cite{Lamport05GeneralizeConsensus}. Moreover, we show how
several well known and practical algorithms can be cast and optimized in our
framework, providing a set of TLA+ specifications that ease the design and model
checking of algorithms based on quorum systems.

TODO\@: expand the following.

Generalized Consensus is an agreement problem relaxing the ordering constraints
imposed on commuting commands and Generalized Paxos \cite{Lamport05GeneralizeConsensus} is an algorithm, similar
in structure to Fast Paxos, which takes advantage of the relaxed ordering
constraint to process commuting commands in two message delays, when three are
needed in the Classic Paxos algorithm.

Generalized Paxos is also structured in a sequence of rounds, 
where each round maintains the agreement property of Generalized Consensus.
In the abstract presentation of Generalized Paxos, Lamport gives a generic and abstract algorithm (not a distributed algorithm, accesses directly the state of all acceptors and rounds) that a round must use to determine the values that are safe to be voted for in the round.
If all round refine this abstract specification, then the resulting algorithm implements Generalized Consensus. Therefore any set of rounds refining Lamport's abstract specification can be combined to form a correct generalized consensus algorithm.  
However, no explicit interface for combining rounds is given and specifying a type of round in isolation from the others would require assume-guarantee reasoning. In contrast, our formulation uses encapsulared rounds and does not require assume-guarantee reasoning.

Vertical Paxos algorithms are SMR algorithms whose set of replica servers can
be reconfigured without stopping to process new commands. Crashed servers can
therefore be replaced by new servers, ensuring the long-term availability of a
replicated service. Vertical Paxos algorithms use an external reconfiguration
master, itself implemented as a replicated state machine, which determines
the servers that should be removed from or added to the configuration. The
role of the reconfiguration master is similar to the role of the scheduling
policy in our framework. In contrast to our work, Vertical Paxos does not define
an interface and correctness condition for encapsulating the reconfigurable
instances of SMR algorithms and therefore does not allow the reuse of existing
algorithms and the incremental development of SMR optimizations.

\section{Generalized Consensus}
\label{sec:gc}

We consider a set of asynchronous servers communicating by message passing in
an asynchronous network. Servers are not necessarily sequential processes.
We describe a behavior of the system using a sequence of states,
where a state is a function from variable names to values (i.e.\ a valuation of
variables). We specify sets of allowed behaviors using an initial predicate
and a next-state relation expressed using unprimed variables and primed variables (next-state variables) in the
style of TLA+ \cite{Lamport02SpecifyingSystems}. However, for succinctness, our
specification are rather informal, eluding non-essential details.

We consider a computing service exposing a set of commands $C$. The
service has a sequential specification consisting of an initial state
and a deterministic transition relation in which every command
atomically updates the state of the service and produces an output. We
assume that duplicate commands have no effect on the state of the
service and return the same output as did their first occurrence.

The goal of SMR is to provide a reliable implementation of the computing service despite server crashes.
To do so, a SMR algorithm replicates the execution of the service on several servers. 
Traditionally, the servers play one or
several of three roles: \emph{proposers}, which propose new commands to execute,
\emph{acceptors}, whose role is to order commands, and
\emph{learners}, which learn about the growing sequence of commands to
execute by querying the acceptors \cite{lamport2001paxos}. Often, but not necessarily, every replica server plays the
three roles of proposer, acceptor, and learner: clients of the service
send their commands to some replica servers; replica servers submit
the received commands to the SMR protocol in the role of proposers;
replica servers respond to commands by executing the commands learned
in the role of learners; replica servers participate in command
ordering in the role of acceptors.  A SMR algorithm guarantees that
the replicas learn the same sequences of commands, possibly up to the
reordering of commuting commands. Therefore, every learner can respond to client
requests using its last learned sequence of commands, and clients will receive
the same responses as the ones produces by a centralized, non-replicated,
service. 

\subsection{Histories}

To simplify the specification of SMR algorithms optimizing the execution of commuting commands 
we will use the notion of a command history, or history for short, in our specification of Composable Generalized Consensus.

A history is a data structure that
represents a set of sequences which are the same modulo the reordering
of commuting commands and addition or removal of duplicate commands.
We say that two commands $c_1$ and $c_2$ \emph{commute} when, in every
sequential execution of the service in which $c_1$ and $c_2$ are adjacent,
reversing the order of $c_1$ and $c_2$ does not change any of the outputs,
including those given to $c_1$ and $c_2$.

By definition, all the sequences represented by a given history contain the same commands
and determine the same outputs for each command. Therefore, to implement a
replicated service, it is sufficient that learners learn histories instead of
sequences. 

Command histories are built from the empty history, noted $\bot$, using the $\star$
operator, which appends a sequence of commands $cs$ to a history $h$, obtaining
the history $h \star cs$. We define a partial order relation among
histories: $h_1 \leq h_2$ when there exists a sequence $cs$ such that $h_2 = h_1
\star cs$. Histories have the property that every finite set $H$ of histories
has a \emph{greatest lower bound}, noted $GLB\left(H\right)$. 
We say that a history $h$ contains the command $c$ when there exists a sequence of commands 
$cs$ containing $c$ such that $\bot \star cs = h$.
Finally, we say
that a set of histories $H$ is compatible when all the histories in $H$ have a
common upper bound $h$ such that $h$ can be built using only commands contained in
the histories in $H$.
%The maximum over a set of histories $H$ is denoted by $Max\left( H \right)$ and $Max\left( \aset{} \right)$ is the empty history.

The major advantage that histories have over sequences is that if a sequence
$cs_1$ is obtained by reordering the commuting commands of $cs_2$, then $h \star
cs_1 = h \star cs_2$. Therefore, in SMR implementations, servers can agree on
a history even though they chose different orders for some commuting commands.

For a thorough discussion of command histories we refer the reader
to \cite{Lamport05GeneralizeConsensus}, where histories are called C-structs. 
%Our Isabelle/HOL formalization abstracts over the concrete representation of histories, assuming only their
%necessary properties. 
Command histories were first introduced in trace theory by Mazurkiewicz 
\cite{Mazurkiewicz84Semantics}.


\subsection{Generalized Consensus}

Generalized Consensus precisely specifies the allowed behaviors of the roles of \emph{proposers}
and \emph{learners}, abstracting over the behavior of acceptors. Acceptors are used to implement Generalized Consensus but do not appear in its interface.

We specify Generalized Consensus using the variables $proposed$, containing the proposed commands so far,
and, for every learner $l$,  a variable $learned\left[ l \right]$, a set containing the histories learned by $l$ so far. Initially, $proposed = \aset{}$ and, for every learner $l$,
$learned\left[ l \right] = \aset{}$. 

The next-state relation consists of two types of transitions:
$Propose\left( l, c
\right)$ transitions, for a learner $l$ and a command $c$, in which
$c$ is inserted in the set $proposed$; $Learn\left( l,h \right)$
transitions, for a learner $l$ and a history $h$, in
which $h$ is inserted in the set $learned\left[ l \right]$. 
A $Learn\left( l, h \right)$ transition is guarded by the following conditions:
\begin{compactitem}
    \item[\textbf{Agreement}:] The set of learned histories after the transition, $\aset{\bigcup learned'\left[ l
      \right] : l \in Learners}$, is compatible.
    \item[\textbf{Validity}:] Every learned history contains only
        proposed commands.  
    \item[\textbf{Irrevocability}:] The new learned history
      $h$ is an extension of every history in $learned\left[ l \right]$. 
        %\\ $cs > Max\left(learned\left[ l \right]\right)$.
\end{compactitem}


When clients of a replicated service submit their commands to the proposers and learners compute the response to give to clients using the maximum over the histories in $learned\left[ l \right]$, the three properties above imply that the clients see a \emph{linearizable} execution
\cite{HerlihyWing90LinearizabilityCorrectnessConditionConcurrentObjects}.

\section{Modular Implementations of Generalized Consensus}

In this section we introduce our framework by example, building a modular Generalized Consensus algorithm inspired from Fast Paxos and Generalized Paxos by first designing a fast but fragile modular algorithm, then combining it unchanged with a more resilient algorithm. 

\subsection{The Broadcast-Decide Algorithm}

In our first algorithm, Broadcast-Decide, learners each maintain a variable $accepted\left[ l \right]$ which contains the currently accepted history of learner $l$.
For every learner $l$, $accepted\left[ l \right]$ is initialized to the empty history.
A proposer broadcasts its proposal to all the learners which unconditionally accept it by appending the proposal to their $accepted\left[ l \right]$
history and by broadcasting to all the acceptors their new accepted history, $accepted'\left[ l \right]$.

Each learner $l$ maintains a variable $learned\left[ l \right]$, initially the empty history, containing its last learned history. A learner $l$
learns a new history $h$ when $h \geq learned\left[ l \right]$ and $h$ is a lower bound of the set consisting of the largest history received
from each acceptor.

More precisely, the Broadcast-Decide algorithm is described by the transition relation obtained as the disjunction of the
following transitions. We do not model the network explicitly using state variables but instead we just say that servers send, broadcast, or receive messages.

\begin{compactitem}

\item A $Propose\left( p,c \right)$ transition, for a proposer $p$ and a command $c$, not guarded, broadcasts the message $\aseq{\quo{prop},c}$ to all
the acceptors.

\item An $Accept\left( a \right)$ transition, for an acceptor $a$, enabled when $a$ can receive a $\aseq{\quo{prop},c}$ message from a proposer,
updates $accepted\left[ a \right]$ to $accepted\left[ a \right]\star \aseq{c}$ and broadcasts the message $\aseq{\quo{accepted}, a, accepted'\left[ a \right]}$ to all the learners
($accepted'\left[ a \right]$ is the new value of the variable $accepted\left[ a \right]$).

\item A $Learn\left( l, h \right)$ transition, for a learner $l$ and a history $h$, enabled when $h \geq learned\left[ l \right]$ and $h$ is a lower
bound of the set $\aset{MaxReceivedFrom\left( a \right) : a \in Acceptors}$, where $MaxReceivedFrom\left( a \right)$ is the maximal accepted history received
from the acceptor $a$.
 Its effect is to set $learned\left[ l \right]$ to $h$.

\end{compactitem}

The algorithm Broadcast-Decide satisfies the specification of Generalized Consensus, however it cannot learn new histories if two acceptors $a_1$ and $a_2$ have two histories which are not compatible: since an acceptor can only append new proposals to its accepted history, all the possible future lower bounds of the set $\aset{MaxReceivedFrom\left( a \right) : a \in Acceptors}$ are bounded above by $GLB\left( \aset{accepted\left[ a_1 \right], accepted\left[ a_2 \right]} \right)$.
This situation occurs when non-commuting commands contend: for example, when two different proposers send two commands $c_1$ and $c_2$ that do not commute, the acceptors $a_1$ accepts $c_1$ before $c_2$, and the acceptor $a_2 \neq a_1$ accepts $c_2$ before $c_1$.

To overcome this deadlock, we will make some modifications to the algorithm that will allow us to start a fresh instance $B_2$, called a round, of the Broadcast-Decide algorithm.
To allow switching to a fresh new round of the Broadcast-Decide algorithm, we introduce the new role of switcher, that any server can play additionally from being a proposer, acceptor, or learner.
A switcher will extract an accepted history from the first round and try to initialize all the acceptors of the second round with the extracted history.
If we are luckier than in the first round, there may be no contention between switchers and every acceptor $a$ will start accepting new proposals in the second round based on the same initial history.

The first round $B_1$ and second round $B_2$ each have their own copies of the arrays $accepted$ and $learned$, noted $accepted_1$, $accepted_2$, $learned_1$, and $learned_2$, when the round is not clear from the context. 
We assume that network messages are tagged with a round number and that a message tagged with $i\in\aset{1,2}$ can only be received in $B_i$, creating two isolated logical networks. 

In our example with the deadlock, switchers have the role of initializing the new round of $B_2$ so that it can resume the execution and overcome the deadlock.
We modify the Broadcast-Decide algorithm by introducing new transitions and modifying the $Accept\left( a \right)$ transition as follows.
We add the new variables $status\left[ a \right]$ for every acceptor $a$, which are initialized to $\quo{idle}$, meaning that an acceptor has not been initialized. The purpose of $status\left[ a \right]$ is to prevent the acceptor $a$ from accepting commands until its status changes from $\quo{idle}$ to $\quo{ready}$ and to stop it when its status goes to $\quo{stopped}$.
We also add the new arrays of variables $inits\left[ s \right]$ and $aborts\left[ s \right]$, for every switcher $s$. The variables $inits\left[ s \right]$ are initialized to the empty history in $B_1$ and to $\quo{none}$ in $B_2$, while the variables $aborts\left[ s \right]$ are initialized to $\quo{none}$ in both $B_1$ and $B_2$. The purpose of the arrays of variables $aborts$ and $inits$ is to transfer state between $B_1$ and $B_2$. The arrays of variables $aborts$ and $inits$ form what we call the \emph{inter-round} interface.
The rounds $B_1$ and $B_2$ each have their own copy of the arrays of additional variables $status$, $inits$, and $aborts$, and we add subscripts denoting the round when the round is not clear from the context. 

\begin{compactitem}
\item We modify the $Accept\left( a \right)$ transition by enabling it only if $status\left[ a \right] = \quo{ready}$ and by having the acceptor $a$ additionally broadcast its new accepted value to the switchers.
Switchers detect that the algorithm is deadlocked when they have received two messages $\aseq{\quo{accepted}, a_1, h_1}$ and $\aseq{\quo{accepted}, a_2, h_2}$ where  $h_1$ and $h_2$ are incompatible.

\item The $Stop\left( s \right)$ transition, for a switcher $s$, is enabled when the switcher has detected a deadlock or a crash (for example using a timeout). Its effect is to broadcast a $\aseq{\quo{stop}, s}$ message to the acceptors.

\item The $Stop\left( a \right)$ transition, for an acceptor $a$, is enabled when the acceptor $a$ can receive a $\aseq{\quo{stop}, s}$ message from a switcher $s$.
  Its effect is to update $status\left[ a \right]$ to $\quo{stopped}$ and to send the message $\aseq{\quo{stopped},a,accepted\left[ a \right]}$ to the switcher $s$.

\item The $Abort\left( s, h\right)$ transition, for a switcher $s$, is enabled when $aborts\left[ s \right] = \quo{none}$ and the switcher has received
  at least one $\aseq{\quo{stopped}, a, h}$ message. Its effect is to set the variable $aborts\left[ s \right]$ to the history $h$ received in the $\quo{stopped}$ message. The history $aborts'\left[ s \right]$ is then called an \emph{abort history}. 

\item The $Init\left( s,h \right)$ transition, for a switcher $s$, is enabled when $inits\left[ s \right]=\quo{none}$ and $h$ is a history, and its effect is to set $inits\left[ s \right]$ to $h$, and to broadcast the message $\aseq{\quo{init}, h}$ to the acceptors.  The history $inits\left[ s \right]$ is then called an \emph{init history}.
As described below, in $B_2$ composed with $B_1$, this history is constrained to be an abort history of $B_1$.

 \item The $WakeUp\left( a \right)$ transition, for an acceptor $a$, is enabled when $status\left[ a \right]$ is $\quo{idle}$ and $a$ can receive a $\aseq{\quo{init}, h}$ message from a switcher. Its effect is to set set $status\left[ a \right]$ to $\quo{ready}$ and $accepted\left[ a \right]$ to $h$.

\end{compactitem}

The composition $B_1\times B_2$ of the two rounds $B_1$ and $B_2$ is obtained as follows:
\begin{itemize}
  \item We Require that a transition of the composition be a transition of either specification but not both, except for the $Abort\left( s \right)$ and $Init\left( s \right)$ transitions, which are allowed to take place at the same time. 
  \item We require that $aborts_1 = inits_2$ at all times.
\end{itemize}
The composition models two independent rounds being linked only by the switchers passing the abort histories of $B_1$ to $B_2$, where they become init histories: switchers gather abort histories in $B_1$ thanks to the $Stop$ transitions, then they broadcast them as init histories to the acceptors in $B_2$. The acceptors then initialize their state to a received init history in the $WakeUp\left( a \right)$ transition.
This process can be repeated as often as needed to switch to new rounds $B_3$, $B_4$, $B_5$, etc., each time a round cannot learn new histories anymore.

Crucially, the composition of any number of rounds implements Generalized Consensus.  To understand why, observe that in an $Abort\left( s,h \right)$ transition, the switcher $s$ has received a $\aseq{\quo{stopped},a,h}$ message from the acceptor $a$, which means that the acceptor $a$ no longer accepts new proposals and that $h$ is the current value of the $learned\left[ a \right]$ variable. Therefore no history greater than $h$ can ever be learned anymore in this round. An abort history is thus a safe
starting point for a new round to resume execution. Since a new round of the Broadcast-Decide algorithm guarantees that a single init history will be used as a prefix a new learned histories, the properties of Generalized Consensus are preserved. 

The Broadcast-Decide algorithm can no more learn new histories whenever one acceptor crashes because the crashed acceptor prevents the lower bound of the accepted histories to ever increase. However, the round-change mechanism is very resilient to faults and can abort anyway thanks to the timeout triggering the $Stop\left( s \right)$ transition: a single live acceptor with a single live switcher can abort the execution and change round.

To allow a new round to make progress after some acceptors crashed in a previous round, we introduce a component called the switching policy. With the Broadcast-Decide rounds, the task of the switching policy is to update the set of acceptors from one round to the next, ensuring that failed acceptors are removed or replaced by live ones. The switching policy must be queried by the switchers upon changing round to determine the new set of acceptors. This information can then be piggybacked to the
acceptors and learners, or the acceptors and learners can directly query the switching policy. 
The set of switchers can be changed from round to round in the same fashion. Moreover, in an algorithm composed of several different types of rounds, the switching policy will also be responsible for choosing the type of round to execute next.

\subsection{The Leader Algorithm} 
\label{sec:leader}

The Broadcast-Decide algorithm can process a proposal in two message delays when there are no conflicts, i.e.\ no contention on non-commuting commands. However, if there are repeated conflicts, the algorithm may not make progress despite changing round, as the acceptors of the each new round
may be initialized with incompatible histories. The Fast Paxos algorithm \cite{Lamport06FastPaxos} is able to learn new histories under high contention by
designating a leader server that will totally order the proposals. However, to make our Broadcast-Decide algorithm work like Fast Paxos, we would have to modify it substantially. If we had spent effort testing Broadcast-Decide implementations and proving it correct, this effort would be wasted and we would have to redo all the tests and proofs from scratch.

Instead, we will now show how to obtain an algorithm with performance characteristics close to those of Fast Paxos, employing a leader to deal with contention, without modifying at all our existing Broadcast-Decide algorithm.
To do so, we define a new kind of rounds, the Leader rounds, which uses the same inter-round interface as the Broadcast-Decide algorithm. 
The inter-round interface will allow us to combine Leader rounds and Broadcast-Decide rounds in any order, obtaining an algorithm which can adapt its strategy to its environment, selecting the Broadcast-Decide algorithm under low contention and switching to the slower Leader algorithm when there is contention.

In the Leader algorithm, the proposers send their proposals to a distinguished acceptor called the leader. 
The leader totally orders the proposals it receives and forwards them to all the acceptors which accept them in the order defined by the leader.
As in the Broadcast-Decide algorithm, acceptors broadcast their accepted histories to the learners, which learn a new history when it is a lower bound of 
the set $\aset{MaxReceivedFrom\left( a \right) : a \in Acceptors}$.
Thanks to its leader, a Leader round continues to learn new histories even when proposals conflict because the leader ensures that the accepted histories of the acceptors remain consistent. 

The Leader algorithm treats all proposals equally and does not take advantage of commutativity. When there are no server crashes, each proposal is processed in three message delays. In comparison, in the absence of crashes, the Broadcast-Decide algorithm processes non-conflicting proposals in two message delays but can take arbitrarily long to process conflicting proposals.

To initialize a Leader round, the switchers send their init histories to the leader, which chooses a unique one that it forwards to the acceptors, guaranteeing that all the acceptors are initialized with the same history even when conflicting init histories are sent concurrently.

The Leader algorithm can not learn new histories if its leader, or any other acceptor, crashes.
To recover from this deadlock we pass the baton to a new round, as in the Broadcast-Decide algorithm, with a similar mechanism: a switcher that times-out waiting for new messages from the acceptors broadcasts a stop message, waits for receiving the accepted history of an acceptor, and finally aborts using the received history as abort history. 

Let us specify more precisely the Leader algorithm.
The specification uses the same variables as the Broadcast-Decide algorithm and starts in the same initial state. 
The next-state relation is the disjunction of the following types of
transitions. 
\begin{compactitem}
\item A $Propose\left( p,c \right)$ transition, for a proposer $p$
  and a command $c$, not guarded, sends the message $\aseq{\quo{prop},c}$ to
  the leader.
\item An $Init\left( s, h \right)$ transition, for a switcher $s$
  and a history $h$, is enabled when $inits\left[ s \right] = \quo{none}$, and its effect is to set $inits\left[ s \right]$ to $h$ and
  to sends the message $\aseq{\quo{init},h}$ to the leader.
\item A $WakeUp\left( leader \right)$ transition, enabled when
  $status\left[ leader \right]$ is ``idle'' and 
  a message $\aseq{\quo{init},h}$ was sent to the leader. Its effect is to
  receive the message, set $accepted\left[ leader \right]$ to $h$,
  to set $status\left[ leader \right]$ to ``ready'', and to
  broadcast the message $\aseq{\quo{leader-accept},h}$ to all
  other acceptors.
\item An $Accept\left( leader \right)$ transition, enabled when
  $status\left[ leader \right]$ is ``ready'' and a $\aseq{\quo{prop},c}$ message has been sent to the leader. The effect of the
  action is to receive the message, to update $accepted\left[ leader
  \right]$ to $accepted\left[ leader \right]\star c$,  to broadcast the message
  $\aseq{\quo{leader-accept},accepted'\left[ leader \right]}$
  to all the other acceptors, and to
  broadcast the message $\aseq{\quo{accepted},accepted'\left[ leader
  \right]}$ to the learners.
\item An $Accept\left( a \right)$ transition, for an acceptor $a$
  which is not the leader, is enabled when $status\left[ a
  \right]$ is ``ready'' and the leader has sent a
  $\aseq{\quo{leader-accept},h}$ message to $a$, where $h > accepted\left[ a
  \right]$. The effect of the action is to receive the message,
  to update $accepted\left[ a \right]$ to $h$, and to broadcast the
  message $\aseq{\quo{accepted},h}$ to the
  learners.
\item A $Learn\left( l, h \right)$ transition, for a learner $l$
  and a history $h$, is enabled when $h \geq learned\left[ l \right]$ and $h$ is a lower
  bound of the set $\aset{MaxReceivedFrom\left( a \right) : a \in Acceptors}$, where $MaxReceivedFrom\left( a \right)$ is the maximal accepted history received
  from the acceptor $a$.  Its effect is to set $learned\left[ l \right]$ to $h$.
\item A $Stop\left( s \right)$ transition, for a switcher $s$,
  enabled when the switcher $s$ has detected a crash (for example using a timeout), broadcasts a $\aseq{\quo{stop}, s}$ message to all
  the acceptors. 
\item A $Stop\left( a \right)$ transition, for an acceptor $a$,
  enabled when $a$ can receive a $\aseq{\quo{stop},s}$ message
  from a switcher $s$. Its effect is to receive the message, set
  $status\left[ a \right]$ to ``stopped'', and to send the
  message $\aseq{\quo{stopped},a,accepted\left[ a \right]}$ to the
  switcher $s$. 
\item An $Abort\left( s, h \right)$ transition, for a switcher $s$ and a history $h$, is enabled when $aborts\left[ s \right] = \quo{none}$ and
  the switcher has received at least one $\aseq{\quo{stopped}, a, h}$ message. Its effect is to set the variable $aborts\left[ s \right]$ to the
  history $h$ received in the $\quo{stopped}$ message.
\end{compactitem}

Thanks to the inter-round interface, which is the same for Leader rounds and Broadcast-Decide rounds, new Leader rounds and Broadcast-Decide rounds can be repeatedly composed in a sequence. 

Observe that a Leader round maintains a similar invariant, relating abort histories to learn histories, than the BroadCast-Decide algorithm:
the learned histories are necessarily smaller than the abort histories in the partial order on histories. In both types of rounds, an abort history is
a safe starting point for initializing the variables $accepted\left[ a \right]$ of the acceptors. Moreover, both types of rounds guarantee that only a
single history will be used as a basis for the learners to learn new histories. These common invariants make the two kinds of rounds compatible, so that we can compose them and obtain an algorithm implementing Generalized Consensus.

In the next section we present Composable Generalized Consensus, a specification of the behavior of rounds at the level of proposals, learned histories, init histories, and abort histories, which allows making the informal reasoning above precise.
Our composition theorem ensures that any set of rounds individually satisfying Composable Generalized Consensus can be composed in any order to yield an algorithm satisfying Composable Generalized Consensus. Therefore we can devise many other types of rounds and compose them unchanged with our two BroadCast-Decide and Leader rounds.

\section{Composable Generalized Consensus}
\label{sec:aca}

We consider a set of servers playing one or multiple of the four roles of proposer, acceptor, learner, and switcher. 
Composable Generalized Consensus specifies the allowed behaviors of the proposers, learners, and switchers
\emph{of a single round}. 
To specify Composable Generalized Consensus, we use the variable $proposed$, a set of proposals, the array of set of histories $learned\left[ l \right]$, where $l$ ranges of learners, the variable $inits$, a set containing the init histories received from the previous
round, and the variable $aborts$, a set containing the abort histories transfered to the next round.
Initially all variables are empty sets.

The next-state relation
consists of four types of transitions. The first two, $Propose\left( l, c \right)$, and $Learn\left( l, h \right)$, for a learner $l$ and a
history $h$, have the same effect as in the specification of Generalized Consensus: 
$Propose\left( l,c \right)$ adds $c$ to the set $proposed$ and $Learn\left( l,h \right)$ adds $h$ to the set $learned\left[ l \right]$.

Composable Generalized Consensus introduces two new transitions: $Init\left( s, h \right)$ and $Abort\left( s, h
\right)$, for a switcher $s$ and a history $h$. The transition $Init\left( s,h \right)$ inserts $h$ in the set $inits$, and models the switcher $s$ receiving
the abort history $h$ from the preceding round. The transition $Abort\left( s, h \right)$ inserts $h$ in the set $aborts$ and models the switcher $s$ passing the abort history $h$ to the next round.  
All the transition are guarded by the following conditions:
\begin{compactitem}
    \item[\textbf{Agreement}:] The set $\aset{\bigcup learned'\left[ l
      \right] : l \in Learners}$ is compatible.
    \item[\textbf{Validity}:] There is a history $h$ belonging to $inits$ such that every abort history or learned history 
        is of the form $h \star cs$, where $cs$ contains only commands
        of the set $proposed$.
    \item[\textbf{Initialization}:] If $inits$ is empty, then no
        history can be learned and no switcher can abort.
    \item[\textbf{Irrevocability}:] In the case of a $Learn\left( l,h
        \right)$ transition, the new learned history $h$ is an 
        extension of every history in $learned\left[ l \right]$.
    \item[\textbf{Safe Abort}:] Every abort history $h$ is an upper bound of the set of learned histories.
\end{compactitem}

Note that if a round is initialized with empty abort histories, then it implements Generalized Consensus.

We compose two rounds of Composable Generalized Consensus as we did in the preceding section.
We write $proposed_2$, $learned_2$, $inits_2$, and $aborts_2$ for the variables of the first round and  $proposed_2$, $learned_2$, $inits_2$, and $aborts_2$
for the variables of the second round.
The transition relation of the composition is the disjunction of the following actions, conjoined with the condition that $aborts_1=inits_2$:
\begin{itemize}
  \item A $Propose$ or $Learn$ action of round 1, leaving the variables of round 2 unchanged.
  \item A $Propose$ or $Learn$ action of round 2, leaving the variables of round 1 unchanged.
  \item An $Init$ action of round 1, leaving the variables of round 2 unchanged.
  \item An $Abort$ action of round 2, leaving the variables of round 1 unchanged.
  \item A joint action consisting of an $Abort$ action of round 1 together with an $Init$ action of round 2.
\end{itemize}

\subsection{The Composition Theorem}

\begin{theorem}[Composition Theorem]
    \label{thm:comp}
Consider the composition of two rounds as defined above.
Define the state variable $proposed$ as $proposed_1 \cup proposed_2$. For every learner $l$, define 
$learned\left[ l \right]$ as the maximum of $learned_1\left[ l \right]$ and $learned_2\left[ l \right]$.
For every switcher $s$, define $inits\left[ s \right]$ as $inits_1\left[ s \right]$ and $aborts\left[ s \right]$ as $aborts_2\left[ s \right]$.

Then the sequence of states of the variables $proposed$, $learned$, $inits$, and $aborts$ satisfy the specification of Composable Generalized Consensus.
\end{theorem}

The composition theorem can be applied recursively to show that the composition of any number of rounds which individually and independently satisfy Composable Generalized Consensus itself satisfies Composable Generalized Consensus. 
Therefore SMR algorithms implemented as rounds in our framework can be reused unmodified and optimized by adding new types of rounds incrementally, building on existing work instead of having to redo all the tests and proofs from scratch every time one adds a new optimization to an existing algorithm.

However, it may not be clear that most SMR algorithms can be expressed as the composition of one or more types of rounds satisfying Composable Generalized Consensus.

We provide a mechanically-checked proof of the composition theorem in Isabelle/HOL at \url{http://losa.fr/cgc}.
The proofs uses a version of the specification of CGC that has a more
executable form that presented in this section. 
The Isabelle/HOL formalization of CGC uses the theory of I/O automata, which
is also formalized in Isabelle/HOL\@. The proof uses a refinement
mapping from the composition of two consecutive CGC instances to a single CGC
instance.

\begin{comment}
\begin{theorem}[Composition Theorem]
    \label{thm:comp}
    In every execution of a succession of CGC instances, the history of valuations
    of the variable $proposed$ and, for every learner $l$, of the variables
    $learned\left[ l \right]$ satisfies Generalized Consensus.
\end{theorem}
\Cref{thm:comp} can be shown by induction on the number of rounds,
using the following lemma.
\begin{lem}
  \label{lem:comp}
    Consider two rounds numbered $i$ and $i+1$. In every execution of a
    succession of CGC instances, the history of valuations of the variables $inits
    = inits_i$, $propose = proposed_i \cup propose_{i+1}$, for every learner
    $l$, $learned\left[ l \right] = learned_i\left[ l \right] \cup
    learned_{i+1}\left[ l \right]$and $aborts = aborts_{i+1}$ satisfies CGC.
\end{lem}
\begin{proof}
  A mechanically-checked proof in Isabelle/HOL appears in \cref{sec:isaproofs}.
  The proofs uses a version of the specification of CGC that has a more
  executable form that presented in this section. Moreover,
  instead of history of commands, the Isabelle/HOL specification uses the more
  general notion of command history, presented in the next subsection.
  The Isabelle/HOL formalization of CGC uses the theory of I/O automata, which
  is also formalized in the Isabelle/HOL\@. The proof uses a refinement
  mapping from the composition of two consecutive CGC instances to a single CGC
  instance.
\end{proof}

\Cref{thm:comp} guarantees that the successive composition of any
number of CGC instances implements Generalized Consensus.
Therefore, with CGC, one can easily build replicated services that
dynamically adapt to the current operating conditions of the system: a
replicated service has several types of rounds at its disposal, each being
tailored to particular conditions. Each time the conditions change,
the current CGC instance aborts and passes the baton to a more 
appropriate type of round, as determined by the scheduling policy. Moreover, a new type
of CGC instance can always be devised after the system is deployed and added on the fly to
the existing types of rounds.

We now describe two important practical optimizations of CGC.


\subsection{Optimizing the Execution of Read-Only Commands}
\label{sec:readonly}

Generalized Consensus can be further optimized by treating 
read-only commands specially, as in Generalized Lattice Agreement
\cite{FalerioETAL12GeneralizedLatticeAgreement}.

We consider the case in which a set of servers each play the four
roles of proposer, acceptor, learner, and switcher.
Clients sends their commands to any server, which proposes them as a
proposer, and records the command as pending.
%When a server learns a history that contains one of its pending
%commands, it uses the history to determine the response to the pending
%command and forwards the response to the appropriate client.  

As observed in \cite{FalerioETAL12GeneralizedLatticeAgreement}, read-only
commands can be executed faster by directly reading from the last learned
history, instead of proposing the read commands and waiting for it to appear in
a future learned history.  Because learned histories form a chain, all the reads
return consistent values and this approach yields a \emph{serializable}
implementation of the service.

However linearizability is not satisfied if reads are served directly
from the last learned history: a lagging replica server could respond
to a read using a history which does not contain all completed update
commands. To make the executions linearizable without resorting to
proposing read-only commands as normal commands, we can employ the same
technique as in \cite{FalerioETAL12GeneralizedLatticeAgreement}: upon
receiving a read command, a server proposes a special no-op command
that commutes with all other commands; the server then responds to the
read command using the first learned history containing the no-op
command. This ensures that all the commands that had completed before
the read command was issued are present in the history.  Moreover,
since no-op commands commute with all other commands, they can be
treated more efficiently than regular read commands by the acceptors,
like, for example, in a fast round of Fast Paxos.

\end{comment}


\section{Fast and Resilient Rounds}
\label{sec:rounds}

In this section we demonstrate how a large class of SMR algorithms, those based on quorum systems \cite{GuerraouiVukolic10RefinedQuorumSystems}, can be expressed as the composition of one or more Composable Generalized Consensus rounds. 
We propose two specifications, \emph{resilient CGC
rounds} and \emph{fast CGC rounds}, which can be refined to obtain executable rounds.
For example, CGC versions of Classic Paxos and Chain
Replication can be obtained by refining the specification of
resilient CGC round, and CGC versions of the fast rounds of Fast Paxos can be obtained by refining the specification of fast CGC rounds.
We provide TLA+ specifications precisely demonstrating how to do so. The refinements have been checked with the TLC model-checker.
The specifications are available at \url{http://losa.fr/cgc}.

Fast and resilient CGC round are two specifications, at an
intermediate level of abstraction, which satisfy CGC\@. Both
specifications describe how the state of the acceptors should be
updated, using the notion of quorum, but abstract over the particular communication topology used
to implement the state accesses: the specifications use a global state
which is accessible by all servers in all roles. One can refine fast
or resilient rounds by implementing the state accesses and coordination
using the network, obtaining a concrete algorithm.

Resilient and fast CGC round are specified using the variables $proposed$, $aborts$, $inits$, $learned\left[ l \right]$, for each learner $l$, 
and, for every acceptor $a$, the variable
$status\left[ a \right]$, which can be either ``idle'', ``ready'', or
``stopped'' and $hist\left[ a \right]$, the local accepted history of the
acceptor $a$, which can be either a history or the special value $None$.
Initially, $inits$ and $aborts$ are empty sets, and, for every learner
$l$, $learned\left[ l \right]$ is the empty history, and, for every acceptor $a$,
$status\left[ a \right]$  is ``idle'' and $hist\left[ a \right]$ is the special
value $None$.

The next-state relations of resilient and fast rounds are composed
of the following types of transitions. We use the notion of \emph{learn quorum} and \emph{abort quorum}, defined shortly after.
\begin{compactitem}
    \item A $Propose\left( p,c \right)$ transition, for a proposer $p$
        and a command $c$, not guarded, inserts $c$ in the set
        $proposed$. 
    \item An $Init\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, not guarded, inserts $h$ in the set $inits$.
    \item A $WakeUp\left( a \right)$ transition, for an acceptor $a$,
        is enabled when $status\left[ a \right]$ is ``idle''. Its
        effect is to set $hist\left[ a \right]$ to a history found in
        $inits$ and to set $status\left[ a \right]$ to ``ready''.
    \item An $Accept\left( a \right)$ transition, for an acceptor $a$,
        is enabled when $status\left[ a \right]$ is ``ready'', and
        updates $hist\left[ a \right]$ to $hist\left[ a \right]\star
        c$, where $c$ is a proposed command. In resilient rounds,
        the transition is additionally guarded by the condition that
        the resulting set of local acceptor histories,
        $\aset{hist'\left[ a \right] : a \in Acceptors}$, forms a chain
        in the partial order $\leq$ (this is typically implemented with a
        leader).
    \item A $Learn\left( l, h \right)$ transition, for a learner $l$
      and a history $h$, enabled when $h \geq learned\left[ l \right]$ and is
      smaller than the GLB of the histories of a learn quorum of acceptors. The effect of
        the transition is to set $learned\left[ l \right]$ to $h$.
    \item A $Stop\left( a \right)$ transition, for an acceptor $a$,
        enabled when $status\left[ a \right]$ is ``ready'', sets
        $status\left[ a \right]$ to ``stopped'' (note that
        $status\left[ a \right] = $ ``stopped'' prevents any further
        ``accept'' transition of acceptor $a$).
%    \item REALLY NEEDED? A $TimeOut\left( a \right)$ transition, for an acceptor $a$,
%        is enabled when $status\left[ a \right]$ is ``idle'' and sets 
%        $status\left[ a \right]$ to ``stopped'' and $hist\left[ a
%        \right]$ to the special value $None$. 
    \item An $Abort\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, is enabled when there exists an abort quorum
        $R$ such that, for every member $a$ of $R$, $status\left[ a
        \right]$ is ``stopped'' and $h \in SafeAborts\left( R
        \right)$. Its effect is to insert  $h$ in the set $aborts$. 
\end{compactitem}

On top of the additional guard on the $Accept$ transition present only
in resilient rounds, resilient and fast CGC round differ in
their definition of learn quorums, abort quorums, and in their
definition of the $SafeAborts\left( R \right)$ operator.
To concisely define read and learn quorums, we define 
\begin{equation}
    Q\left( f \right) = \aset{as \subseteq Acceptors :
        Card\left(as\right) \geq \left\lfloor f\times Card\left(
    Acceptors \right)\right\rfloor + 1}.
\end{equation}

In resilient CGC round, abort and learn quorums are defined such
that the intersection between two learn quorums is nonempty and the
intersection between a learn quorum and an abort quorum is nonempty.
The definition leads to two interesting cases: $LearnQ = AbortQ =
Q\left( 1/2 \right)$; $LearnQ = \aset{Acceptors}$ and $AbortQ =
\aset{\aset{a} : a \in Acceptors}$. The operator $SafeAborts\left( R
\right)$ is defined as the singleton set containing the maximum
element over the local histories of the acceptors in $R$ which are not
$None$. Note that the maximum exists because resilient rounds
guarantee that the histories of the acceptors form a chain.

In fast CGC round, abort and learn quorums are defined such that
the intersection between any two learn quorums is nonempty and the
intersection between any two learn quorums and an abort quorum is
nonempty.  The definition leads to three interesting cases: $LearnQ =
AbortQ = Q\left( 2/3 \right)$; $LearnQ = Q\left( 1/2 \right)$ and
$AbortQ = Q\left( 3/4 \right)$; $LearnQ = \aset{Acceptors}$ and
$AbortQ = Q\left( 0 \right)$.
The operator $SafeAborts\left( R \right)$ is defined as the set of
maximal elements over the set 
\begin{equation}
\aset{GLB\left( \aset{hist\left[ a
\right] : a \in R \cap L \wedge hist\left[ a \right]\neq None} \right)
: L \in LearnQ}.
\end{equation}

The cases in which abort quorums are singleton sets, in both
resilient and fast rounds, allow tolerating
$f$ faults with $f+1$ servers: a round can abort as long as one
acceptor is correct and a new round, possibly using new correct
servers, can take over. Note that changing round cannot happen
without the scheduling policy instructing the switchers about the
identity of the next round and of the new servers replacing the failed ones, and thus tolerating $f$ faults with
$f+1$ servers relies on the assumption of a reliable scheduling policy.

Define, for every learner $l$, the history variable $\overline{learned}\left[ l \right]$ which is initialized to the empty set and which is
updated each time a new local history is accepted by the learner $l$ by
inserting the new local history in $\overline{learned}\left[ l \right]$ (see \cite{AbadiLamport91ExistenceRefinementMappings} for the
definition of a history variable).
\begin{theorem}
  \label{thm:rounds}
    Both resilient and fast CGC round implement CGC under the refinement
    mapping that consists in substituting $\overline{learned\left[ l \right]}$
    for $learned\left[ l \right]$.
\end{theorem}


\begin{comment}
\section{Examples of Concrete CGC Algorithms}
\label{sec:examples}

In this section we show how to modify the rounds of Classic Paxos,
Multi-Coordinated Paxos, Chain Replication, Ring Paxos, and Fast Paxos to obtain
CGC round. \Cref{thm:comp} then guarantee that all those different types of
rounds can be used in the same algorithm to yield an implementation of Chain
Agreement.

\subsection{Classic Paxos}

The rounds of the Classic Paxos algorithm can be modified so as to
obtain an implementation of resilient CGC round.  We define
Classic Paxos CGC round by specializing the specification of
resilient CGC round, giving a concrete protocol for disseminating
the proposed commands to the acceptors and enforcing that the learned
histories form a chain.

Each Classic Paxos round has a unique leader among the acceptors which
centrally orders the proposed commands, which are accepted by the
other acceptors in the order determined by the leader.  This protocol
ensures that every local acceptor history is a prefix of the local
history of the leader, therefore guaranteeing that the local acceptor histories
forming a chain.

Acceptor broadcast each newly accepted history to the learners, which
learn a history $h$ when they receive enough extensions of $h$ from the
acceptors.  When instructed by the scheduling policy to abort, a
switcher sends a stop message to the acceptors, which irrevocably stop
updating their local history and send back to the switcher an
acknowledgement containing their local history. A switcher
determines an abort history once it has received acknowledgments from
an abort quorum of acceptors $R$, using the operator $SafeAborts\left(
R \right)$.

The specification of Classic Paxos CGC round uses the variables of
the specification of resilient CGC round, namely the variables
$proposed$, $inits$, $aborts$, the arrays $learned\left[ l \right]$,
$status\left[ a \right]$, and $hist\left[ a \right]$, and two
additional arrays $accepted\left[ l \right]\left[ a \right]$ and
$stop\left[ s \right]\left[ a \right]$, where $l$ is a learner, $a$ is
an acceptor, and $s$ is a switcher.

There is also a network component that we do not describe explicitly.
However, the network allows any servers to communicate by sending and
receiving messages.

Initially, the variables $proposed$, $inits$, $aborts$, and, for every
learner $l$, $learned\left[ l \right]$ are empty sets. For every
acceptor $a$, $status\left[ a \right]$ is ``idle'' and $hist\left[ a
\right]$ is the special value $None$. For every learner $l$ and acceptor $a$,
$accepted\left[ l \right]\left[ a \right]$ is the special value
$None$. For every switcher $s$ and acceptor $a$, $stopped\left[ s
\right]\left[ a \right]$ is set to $false$.  There are initially no
messages in the network.

The next-state relation is composed of the following types of
transitions, where the definitions of learn quorums, abort quorums,
and of the $SafeAborts\left( R \right)$ operator are the ones of resilient
CGC round.
\begin{compactitem}
    \item A $Propose\left( p,c \right)$ transition, for a proposer $p$
        and a command $c$, not guarded, inserts $c$ in the set
        $proposed$ and sends the message $\aseq{\quo{prop},c}$ to
        the leader.
    \item An $Init\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, not guarded, inserts $h$ in the set $inits$ and
        sends the message $\aseq{\quo{init},h}$ to the leader.
    \item A $WakeUp\left( leader \right)$ transition, enabled when
        $status\left[ leader \right]$ is ``idle'' and the leader can
        receive a $\aseq{\quo{init},h}$ message. Its effect is to
        receive the message, set $hist\left[ leader \right]$ to $h$,
        to set $status\left[ leader \right]$ to ``ready'', and to
        broadcast the message $\aseq{\quo{leader-accept},h}$ to all
        other acceptors.
    \item An $Accept\left( leader \right)$ transition, enabled when
        $status\left[ leader \right]$ is ``ready'' and the leader can
        receive a $\aseq{\quo{prop},c}$ message. The effect of the
        action is to receive the message, to update $hist\left[ leader
        \right]$ to $hist\left[ leader \right]\star c$, and to
        broadcast the message $\aseq{\quo{ack},hist\left[ leader
        \right]\star c}$ to the learners, and to broadcast the message
        $\aseq{\quo{leader-accept},hist\left[ leader \right]\star c}$
        to all the other acceptors.
    \item An $Accept\left( a \right)$ transition, for an acceptor $a$
        which is not the leader, is enabled when $status\left[ a
        \right]$ is ``ready'' and $a$ can receive a
        $\aseq{\quo{leader-accept},h}$ message where $h > hist\left[ a
        \right]$. The effect of the action is to receive the message,
        to update $hist\left[ a \right]$ to $h$, and to broadcast the
        message $\aseq{\quo{ack},hist\left[ a \right]\star c}$ to the
        learners.
    \item A $RcvAck\left( l \right)$ transition, for a learner $l$, is
        enabled when $l$ can receive a message $\aseq{\quo{ack},h}$
        from an acceptor $a$. Its effect is to receive the message and
        to set $accepted\left[ l \right]\left[ a \right]$ to $h$.
    \item A $Learn\left( l, h \right)$ transition, for a learner $l$
        and a history $h$, is enabled when $h > Max\left(
        learned\left[ l \right] \right)$ and there is a learn quorum
        $Q$ of acceptors such that $h = GLB\left( \aset{accepted\left[
        l \right]\left[ a \right] : a \in Q }\right)$. Its effect is
        to insert $h$ in $learned\left[ l \right]$.
    \item A $Stop\left( s \right)$ transition, for a switcher $s$,
        always enabled, broadcasts a $\aseq{\quo{stop}}$ message to all
        the acceptors.  This transition models a switcher being
        instructed by the scheduling policy to abort the current round.
    \item A $Stop\left( a \right)$ transition, for an acceptor $a$,
        enabled when $a$ can receive a $\aseq{\quo{stop}}$ message
        from a switcher $s$. Its effect is to receive the message, set
        $status\left[ a \right]$ to ``stopped'', and to send the
        message $\aseq{\quo{stopped},hist\left[ a \right]}$ to the
        switcher $s$.
    \item A $RcvStopped\left( s \right)$ transition, for a switcher
        $s$, enabled when $s$ can receive a $\aseq{\quo{stopped},h}$
        message from an acceptor $a$. Its effect is to receive the
        message and to set $stopped\left[ s \right]\left[ a \right]$
        to $h$.
    \item An $Abort\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, is enabled when there exists an abort quorum
        $R$ such that, for every member $a$ of $R$, $stopped\left[ s
        \right]\left[ a \right]$ is not $false$ and $h \in
        SafeAborts\left( R \right)$, where $SafeAborts\left( R
        \right)$ is computed using the array $stopped\left[ a
        \right]$. Its effect is to insert  $h$ in the set $aborts$. 
\end{compactitem}
Compared to the rounds of the original Classic Paxos, Classic Paxos CGC
rounds add an extra level of indirection when changing round: in the
original rounds, the leader of the new round directly queries the
acceptors of the preceeding round, whereas in CGC round, the
switchers query the acceptors and then send their abort histories to
the leader. However, changing round is an infrequent event, therefore
this difference should not have much practical impact on performance.
\begin{theorem}
  Classic Paxos CGC round satisfy Composable Generalized Consensus.
\end{theorem}
\begin{proof}
  A Classic Paxos CGC instance implements the specification of Abortable
  Generalized Consensus under the refinement mapping consisting in
  projecting the state of the round onto the variables $proposed$,
  $inits$, $aborts$, and, for every learner $l$, $learned\left[ l
  \right]$.
\end{proof}

\end{comment}

\subsection{Refining Fast and Resilient Rounds}

Classic Paxos \cite{lamport2001paxos}, Multi-Coordinated Paxos \cite{CamargosSchmidtPedone07MulticoordinatedPaxos}, Chain Replication \cite{RenesseSchneider04ChainReplicationSupportingHighThroughputAvailability}, and Ring Paxos \cite{MarandiETAL10RingPaxosHighthroughputAtomicBroadcastProtocol} refine Resilient Rounds, using a leader to ensure that the acceptor's accepted histories remain compatible. They differ in the way the leader is
implemented and in the way that the histories of the leader are disseminated to
the other acceptors.

The rounds of Classic Paxos are similar to the Leader algorithm of \cref{sec:leader} but
tolerate that a minority of servers crash by using majority quorums instead of
waiting for histories from all the acceptors.

Multi-Coordinated Paxos avoids depending on a unique leader by using a
distributed leader implementation that can make progress as long as a strict
majority of the acceptors are correct, trading off resilience for a higher time
complexity.

In Chain Replication, acceptors are arranged in a chain whose head is
the leader of the round. As in Classic Paxos, abort histories and
proposed commands are sent to the leader. However, instead of the
leader directly sending its local history to all the other acceptors,
the local history of the leader is
forwarded along the chain from one acceptor to the other until it
reaches the last acceptor, which sends the history to the learners. 
By spreading the load more evenly on the all the servers, Chain Replication
can achieve higher throughput that Classic Paxos.

Ring Paxos optimizes Chain Replication by having the leader broadcast
commands to the acceptors using IP multicast and only sending hashes
of the commands along the chain. Ring Paxos rounds can be modified to implement
resilient CGC round in a similar way as for Chain Replication.

The fast rounds of Fast Paxos \cite{Lamport06FastPaxos} and Generalized Paxos \cite{Lamport05GeneralizeConsensus} refine Fast Rounds.
They allow the proposers to directly broadcast their proposal to the 
acceptors and make progress as long as non-commuting proposals do not contend.
They use the same quorums as our Fast Rounds to ensure that it is possible to recover from inconsistencies 
in the acceptors histories.
Their structure is similar to the one of our Broadcast-Decide round.

\begin{comment}
\subsection{Fast Paxos}

Fast Paxos is composed of two types of rounds: classic rounds, like in Classic
Paxos, and fast rounds.
The fast rounds can be transformed into
implementations of fast CGC instances. Instead of sending their
commands to a leader, as in Classic Paxos, the proposers of fast
rounds directly broadcast their commands to all the acceptors. As long
as commands are received in the same order by a learn quorum of
acceptors or are received in different orders but commute, commands can be learned in two message-propagation delays instead of three in Classic Paxos.
However, if non-commuting commands are received in different orders by
several acceptors, the local history of the acceptors may not form a
chain anymore and the rounds must abort. The fast rounds of Fast
Paxos implement fast CGC instances.

Fast Paxos CGC instances are specified using the same set of variables as for
Classic Paxos, initialized to the same values.
The next-state relation is composed of the following types of
transitions, where the definitions of learn quorums, abort quorums,
and of the $SafeAborts\left( R \right)$ operator are the ones of fast
CGC instances.
\begin{compactitem}
    \item A $Propose\left( p,c \right)$ transition, for a proposer $p$
        and a command $c$, not guarded, inserts $c$ in the set
        $proposed$ and broadcasts the message $\aseq{\quo{prop},c}$ to
        all the acceptors.
    \item An $Init\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, not guarded, inserts $h$ in the set $inits$ and
        broadcasts the message $\aseq{\quo{init},h}$ to all the acceptors.
    \item A $WakeUp\left( a \right)$ transition, for an acceptor $a$,
        is enabled when $status\left[ a \right]$ is ``idle'' and $a$
        can receive an $\aseq{\quo{init},h}$ message. Its effect is to
        receive the message, set $hist\left[ a \right]$ to $h$, and to
        set $status\left[ a \right]$ to ``ready''.
    \item An $Accept\left( a \right)$ transition, for an acceptor $a$,
        is enabled when $status\left[ a \right]$ is ``ready'' and $a$
        can receive a $\aseq{\quo{prop},c}$ message. The effect of the
        action is to receive the message, to update $hist\left[ a
        \right]$ to $hist\left[ a \right]\star c$, and to broadcast
        the message $\aseq{\quo{ack},hist\left[ a \right]\star c}$ to
        the learners.
    \item The $RcvAck\left( l \right)$ transitions, for a learner $l$,
        the $Learn\left( l, h \right)$ transitions, for a learner $l$
        and a history $h$, the $Stop\left( s \right)$ transitions, for
        a switcher $s$, $Stop\left( a \right)$ transitions, for an
        acceptor $a$, the $RcvStopped\left( s \right)$ transitions,
        for a switcher $s$, and the $Abort\left( s, h \right)$
        transitions, for a switcher $s$ and a history $h$, which are
        all the same as in Classic Paxos CGC instances.
\end{compactitem}

As for Classic Paxos CGC instances, Fast Paxos CGC instances implement CGC
under the refinement mapping consisting projecting the state of the
round onto the variables $proposed$, $inits$, $aborts$, and, for every
learner $l$, $learned\left[ l \right]$.


\end{comment}

\newpage

\printbibliography


\begin{appendix}
  
  \label{sec:isaproofs}
 
%  \includepdf[pages={1},pagecommand={\section{Mechanical Proof of the
%  Composition Theorem in Isabelle/HOL}\label{sec:isaproofs}},scale=1]{../IsabellePDFS/podctheories.pdf}
%  \includepdf[pages={2-},pagecommand={},scale=1]{../IsabellePDFS/podctheories.pdf}
  
\end{appendix}


\end{document}
