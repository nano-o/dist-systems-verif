\input{header.tex}
\input{defs.tex}

\addbibresource{managed.bib}

\title{Composable Generalized Consensus}

\author{
  Rachid Guerraoui\\
  \texttt{rachid.guerraoui@epfl.ch}
  \and
  Giuliano Losa\\
  \texttt{giuliano@losa.fr}
}

\date{}

\begin{document}

\maketitle

\input{abstract3}

\vspace{2cm}
\begin{center}
This is a regular paper.
\end{center}

\newpage

\input{intro}

\input{roadmap}


\section{Related Work}
\label{sec:related}

SMR is often implemented using a sequence of instances of Consensus.
There are numerous examples of consensus algorithms in the literature.
However, Abortable Chain Agreement is based on Generalized Consensus. 
In Generalized Consensus \cite{Lamport05GeneralizeConsensus}, the learners of a
SMR algorithm repeatedly learn new values belonging to a \emph{C-struct set}. A
C-struct can be seen as a data structure encoding  sequences of commands modulo
the ordering of commands that commute. The advantage of Generalized
Consensus over Consensus is that
two servers which receive commuting commands in different orders can still reach
agreement on C-structs without further synchronization. 
Our Abortable Chain Agreement can also use C-structs, with the same advantages
as Generalized Consensus. For simplicity, the presentation in this
paper uses the more concrete notion of \emph{command history} (see
\cref{sec:histories}).

Generalized Lattice Agreement \cite{FalerioETAL12GeneralizedLatticeAgreement} allows more efficient implementations of SMR when
commands are partitioned into read-only commands and write-only commands.
Abortable Chain Agreement supports the same optimizations as Generalized Lattice Agreement.
Notably, read-only commands are treated specially so as to optimize their
executions (see \cref{sec:readonly}).

Vertical Paxos algorithms 
\cite{LamportMalkhiZhou09VerticalPaxosPrimarybackupReplication} are SMR algorithms
whose set of replica servers can be reconfigured without stopping to process new
commands. Crashed servers can therefore be replaced by new servers, ensuring the
long-term availability of a replicated service.
Vertical Paxos algorithms use an external reconfiguration master, itself
implemented as a replicated state machine, which determines the servers that should
be removed from or added to the configuration.
ACA shares some aspects of Vertical Paxos algorithms: the task of the
reconfiguration master is similar to the task of the scheduling policy
component of ACA (see \cref{sec:aca}).
However, in ACA, changing round can be used not only to replace crashed servers
but also to change the algorithm that the servers are running. 
Changing algorithm is not possible in Vertical Paxos.
In both Vertical Paxos and ACA, a reliable reconfiguration master, resp. a
reliable scheduling policy, allows tolerating $f$ crashes with $f+1$ servers.

The Abstract Framework \cite{GuerraouiETAL10Next700BftProtocols} proposes to
build Byzantine Fault-Tolerant algorithms as the succession of abortable rounds
called Abstract instances. Abstract instances must satisfy the Abstract
correctness properties. By construction, the composition of any number of
Abstract instances is a correct SMR implementation. However, the Abstract
Framework uses totally ordered sequence of commands, making difficult the
optimization of the execution of commuting and read-only commands that are
possible with ACA.  Moreover, the Abstract Framework does not identify the
crucial role of the scheduling policy for the long-term resilience of a service.
Abortable Chain Agreement combines the ideas of the Abstract Framework,
Generalized Paxos, Generalized Lattice Agreement, and Vertical Paxos in a
unified abstraction. 

Finally, the Speculative Linearizability framework
\cite{GuerraouiKuncakLosa12SpeculativeLinearizability} was a first
attempt by the authors of the present paper at generalizing the
Abstract Framework and mechanically proving its properties with the
Isabelle/HOL interactive
proofs assistant.  Like the Abstract Framework, the
Speculative Linearizability framework is based on totally ordered
sequences of commands, with the same drawback for the execution of
commuting and read-only commands. Moreover, Abortable Chain Agreement is both
conceptually simpler and subsumes the Speculative Linearizability
property.  Like Speculative Linearizability, Abortable Chain Agreement
and its main property, the composition theorem (\cref{thm:comp}), are
formalized in Isabelle/HOL (see \cref{sec:isaproofs}).

Abortable Chain Agreement is the most complete framework to date for building
adaptive replication algorithms out of heterogeneous rounds, making state of the
art optimization techniques available to algorithm designers, which was not
possible before.

\section{Generalized Consensus}
\label{sec:gc}

We consider a set of asynchronous servers communicating by message passing in
an asynchronous network. Servers are not necessarily sequential processes.
We describe a behavior of the system using an infinite sequence of states,
where a state is a function from variable names to values (i.e. a valuation of
variables). We specify sets of allowed behaviors using an initial predicate
and a next-state relation expressed using unprimed and primed variables in the
style of TLA+ \cite{Lamport02SpecifyingSystems}. However, for succinctness, our
specification are rather informal, eluding non-essential details.

Our specifications all contain by default a state variable $crashed$ and
 a $Crash\left( x \right)$ transition which is always enabled. A server $x$ faithfully
executes the algorithm that is assigned to it until its identifier is added to
the variable $crashed$ by a $Crash\left( x \right)$ transitions.  When the identifier of a
server is in the set $crashed$, all its actions are disabled.  In an execution,
a server that is never added to the set $crashed$ is said correct.

We consider a computing service exposing a set of commands $C$. The
service has a sequential specification consisting of an initial state
and a deterministic transition relation in which every command
atomically updates the state of the service and produces an output. We
assume that duplicate commands have no effect on the state of the
service and return the same output as did their first occurrence.

The goal of SMR is to provide a reliable implementation of the computing service despite server crashes.
To do so, an SMR algorithm replicates the execution of the service on several servers. 
In a traditional SMR algorithm, the servers can be seen has playing one or
several of three roles: \emph{proposers}, which propose new commands to execute,
\emph{acceptors}, whose role is to order commands, and
\emph{learners}, which learn about the growing sequence of commands to
execute \cite{lamport2001paxos} by querying the acceptors. Often, but not necessarily, every replica server plays the
three roles of proposer, acceptor, and learner: clients of the service
send their commands to some replica servers; replica servers submit
the received commands to the SMR protocol in the role of proposers;
replica servers respond to commands by executing the commands learned
in the role of learners; replica servers participate in command
ordering in the role of acceptors.  A SMR algorithm guarantees that
the replicas learn the same sequences of commands, possibly up to the
reordering of commuting commands. Therefore, every learner can respond to client
requests using its last learned sequence of commands, and clients will receive
the same responses as the ones produces by a centralized, non-replicated,
service. 

\subsection{Histories}

To simplify the specification of SMR algorithms optimizing the execution of commuting commands 
we will use the notion of a command history, or history for short, in our specification of Composable Generalized Consensus.

A command history, abbreviated history, is a data structure that
represents a set of sequences which are the same modulo the reordering
of commuting commands and addition or removal of duplicate commands.
We say that two commands $c_1$ and $c_2$ \emph{commute} when, in every
sequential execution of the service in which $c_1$ and $c_2$ are adjacent,
reversing the order of $c_1$ and $c_2$ does not change any of the outputs,
including those given to $c_1$ and $c_2$.

All the sequences represented by a given history contain the same commands
and determine the same outputs for each command. Therefore, to implement a
replicated service, it is sufficient that learners learn histories instead of
sequences. For a thourough discussion of command histories we refer the reader
to \cite{Lamport05GeneralizeConsensus}. Our Isabelle/HOL formalization \cite{}
abstracts over the concrete representation of histories, assuming only their
necessary properties. Command histories were first introduced in trace theory
\cite{Mazurkiewicz84Semantics}.

Command histories can be built from the empty history using the $\star$
operator, which appends a sequence of commands $cs$ to a history $h$, obtaining
the history $h \star cs$. We can then define define an ordering relation among
histories: $h_1 \leq h_2$ when there exists a sequence $cs$ such that $h_2 = h_1
\star cs$. Histories have the property that every finite set $H$ of histories
has a \emph{greatest lower bound}, noted $GLB\left(H\right)$. Finally, we say
that a set of histories $H$ is compatible when all the histories in $H$ have a
common upper bound $h$ such that $h$ can be built using only commands found in
the histories in $H$.
%The maximum over a set of histories $H$ is denoted by $Max\left( H \right)$ and $Max\left( \aset{} \right)$ is the empty history.

The major advantage that histories have over sequences is that if a sequence
$cs_1$ is obtained by reordering the commuting commands of $cs_2$, then $h \star
cs_1 = h \star cs_2$. Therefore, in implementations of CGC, servers can agree on
a history even though they chose different orders for some commuting commands.

\subsection{Generalized Consensus}

Generalized Consensus precisely specifies the allowed behaviors of the roles of \emph{proposers}
and \emph{learners}, abstracting over the behavior of acceptors. 

We specify Generalized Consensus using the state variables $crashed$, $proposed$, containing the proposed commands,
and, for every learner $l$,  $learned\left[ l \right]$, a set of
sequences of commands. Initially, $crashed$, $proposed$ and, for every learner $l$,
$learned\left[ l \right]$ are empty sets. 
Remember that any transition involving a server $x$ is always disabled when $x\in crashed$. 

The next-state relation consists of three types of transitions:
$Crash\left( x \right)$ transitions; $Propose\left( l, c
\right)$ transitions, for a learner $l$ and a command $c$, in which
$c$ is inserted in the set $proposed$; $Learn\left( l,cs \right)$
transitions, for a learner $l$ and a sequence of commands $cs$, in
which $cs$ is inserted in $learned\left[ l \right]$. 
A $Learn\left( l, cs \right)$ transition is guarded by the following
conditions:
\begin{compactitem}
    \item[\textbf{Agreement}:] The set of learned histories after the transition, $\aset{learned'\left[ l
      \right] : l \in Learners}$, is compatible.
    \item[\textbf{Validity}:] Every learned sequence contains only
        proposed commands.  
    \item[\textbf{Irrevocability}:] The new learned sequence
        $cs$ is an extension of $learned\left[ l
        \right]$. 
        %\\ $cs > Max\left(learned\left[ l \right]\right)$.
\end{compactitem}

Thanks to the properties of Generalized Consensus, the clients, observing
their input commands and the received responses, see a
\emph{linearizable} execution
\cite{HerlihyWing90LinearizabilityCorrectnessConditionConcurrentObjects}.

\section{Modular Implementations of Generalized Consensus}

Before formally introducing our framework, let us see how we can build a modular implementation similar to Fast Paxos by first designing a fast but fragile modular GC algorithm, then combining it unchanged with a more resilient module, obtaining an algorithm similar to Fast Paxos. 

\subsection{Fast Generalized Consensus}

In our first algorithm, Fast GC, learners each maintain a variable $hist\left[ l \right]$ which contain the currently accepted history of learner $l$.
For every learner $l$, $accepted\left[ l \right]$ is initialized to the empty history.
A proposer broadcasts its proposal to all the learners which unconditionally accept it by setting appending it to their $accepted\left[ l \right]$
variable and broadcasting to all the acceptors the new value of their accepted history.
Each acceptor $l$ maintains a variable $learned\left[ l \right]$, initially the empty history, containing its last learned history. A learner $l$
learns a new history $h$ when $h \geq learned\left[ l \right]$ and $h$ is a lower bound of the set consisting of the last accepted history received
from each acceptor.

More precisely, the Fast-GC algorithm is described by the sequence of states obtained by the transition relation obtain as the disjunction of the
following transitions. We do not model the network explicitly using state variables but instead we just say that servers send, broadcast, or receive messages.

\begin{compactitem}

\item A $Propose\left( p,c \right)$ transition, for a proposer $p$ and a command $c$, not guarded, broadcasts the message $\aseq{\quo{prop},c}$ to all
the acceptors.

\item An $Accept\left( a \right)$ transition, for an acceptor $a$, enabled when $a$ can receive an $\aseq{\quo{prop},c}$ message from a proposer,
updates $accepted\left[ a \right]$ to $accepted\left[ a \right]\star \aseq{c}$ and broadcasts the message $\aseq{\quo{accepted}, a, accepted'\left[ a \right]}$ to all the learners
($accepted'\left[ a \right]$ is the new value of the variable $accepted\left[ a \right]$).

\item A $Learn\left( l, h \right)$ transition, for a learner $l$ and a history $h$, enabled when $h \geq learned\left[ l \right]$ and $h$ is a lower
bound of the set $\aset{LastReceived\left( a \right) : a \in Acceptors}$, where $LastReceived\left( a \right)$ is the last accepted history received
from the acceptor $a$.

\end{compactitem}

The algorithm Fast GC cannot make progress if two acceptors $a_1$ and $a_2$ have two histories which are not compatible because, since one can only append to the histories $accepted\left[ l \right]$, all the possible future lower bounds of the set $\aset{LastReceived\left( a \right) : a \in Acceptors}$ are bounded above by $GLB\left( \aset{accepted\left[ a_1 \right], accepted\left[ a_2 \right]} \right)$.
This situation occurs when there is contention on non-commuting commands: for example, when two different proposers send two commands $c_1$ and $c_2$ that do not commute, an acceptors $a_1$ accepts $c_1$ before $c_2$, and an acceptor $a_2 \neq a_1$ accepts $c_2$ before $c_1$.

To overcome this deadlock, we will make some modifications to the algorithm that will allow us to start a fresh instance $FGC_2$, called a round, of the Fast GC algorithm.

Basically, we introduce the new role of switcher, that any server can play additionally from being a proposer, acceptor, or learner.
A switcher will extract a maximal accepted history from the first round and try to initialize all the acceptors of the second round with the extracted history.
If we are luckier than in the first round, there may be no contention between switchers and every acceptor $a$ will start accepting new proposals in the second round based on the same initial history $accepted\left[ a \right]$.

We write $FGC_1$ for the first, deadlocked, round of the Fast GC algorithm.
The rounds $FCG_1$ and $FCG_2$ each have their own copies of the arrays $accepted$ and $learned$, noted $accepted_1$, $accepted_2$, $learend_1$, and $learned_2$ when the round is not clear from the context. 
We assume that network messages are tagged with a round number and that a message tagged with $i\in\aset{1,2}$ can only be received in $FCG_i$, creating two isolated logical networks. 

Switchers have the role of initializing the new round of Fast GC so that it can resume the execution and overcome the deadlock.

We modify the FCG algorithm by introducing three new transitions and modifying the $Accept\left( a \right)$ transition as follows.
We add the new variables $status\left[ a \right]$ for every acceptor $a$, which are initialized to $\quo{idle}$. The purpose of $status\left[ a \right]$ is to prevent the acceptor $a$ from accepting commands until its status changes from $\quo{idle}$ to $\quo{ready}$.
We also add the new variables $inits\left[ s \right]$ and $aborts\left[ s \right]$, for every switcher $s$, initialized to the empty history in $FGC_1$ and to $\quo{none}$ in $FGC_2$. The purpose of the arrays $aborts$ and $inits$ is to transfer state between $FGC_1$ and $FGC_2$.
The algorithms $FGC_1$ and $FGC_2$ each have their own copy of the arrays $status$, $inits$, and $aborts$, and we adopt the same convention as above consisting in adding a subscript denoted the round to a variable when the round is not clear from the context. 

\begin{compactitem}
  \item We modify the $Accept\left( a \right)$ transition by having the acceptor $a$ additionally broadcast its new accepted value to the switchers.
Switchers can therefore detect that the algorithm is deadlocked when there are two broadcast messages $\aseq{\quo{accepted}, a_1, h_1}$ and $\aseq{\quo{accepted}, a_2, h_2}$ in the network such that $h_1$ and $h_2$ are incompatible.

\item The $Abort\left( s \right)$ transition, for a switcher $s$, is enabled when the switcher has detected a deadlock. Its effect is to 
   set the variable $aborts\left[ s \right]$ to the maximum of the histories received from $a$, where $a$ is any acceptor. 
   The history $aborts'\left[ s \right]$ is then called an abort history.

 \item The $Init\left( s \right)$ transition, for a switcher $s$, always enabled, and its effect is to to pick a history $h$, set $inits\left[ s \right]$ to $h$, and to broadcast the message $\aseq{\quo{init}, h}$ to the acceptors. In $FGC_2$, composed with $FGC_1$, this history is constrained to be an abort history of $FGC_1$.
   The history $inits\left[ s \right]$ is then called an init history.

 \item The $WakeUp\left( a \right)$ transition, for an acceptor $a$, is enabled when $status\left[ a \right]$ is $\quo{idle}$ and there is at least one switcher $s$ such that $inits\left[ s \right] \neq \quo{none}$. Its effect is to set set $status\left[ a \right]$ to $\quo{ready}$ and $accepted\left[ a \right]$ to $inits\left[ s \right]$.

\end{compactitem}

The two rounds $FGC_1$ and $FGC_2$ are composed by requiring that $aborts_1 = inits_2$ at all times.
This model the switchers passing the histories $aborts\left[ s \right]$ from $FGC_1$ to $FGC_2$, where they broadcast to the acceptors and used in the $WakeUp\left( a \right)$ transitions to initialize the state of the acceptors.
This process can be repeated as often as needed, with new rounds $FGC_3$, $FGC_4$, etc., to try to make progress.

The initial $FGC$ algorithm also enters a deadlock whenever one acceptor crashes because the learners wait for the lower bound over all accepted histories of all acceptors to increase. However, the round-change mechanism is very resilient to faults: a single live acceptor with a single live switcher can abort the execution and change round.
To allow a new round to make progress after some acceptors crashed in a previous round, we introduce a component called the switching policy. With the Fast GC rounds, the task of the switching policy is to update the set of acceptors from one round to the next, ensuring that failed acceptors are removed or replaced by live ones. The switching policy must be queried by the switchers upon changing round to determine the new set of acceptors. This information can then be piggybacked to the
acceptors and learners, or the acceptors and learns can directly query the switching policy too. 
The set of switchers can be changed from round to round in the same fashion. Moreover, in an algorithm composed of several different types of rounds, the switching policy will also be responsible for choosing the type of round to execute next.

\section{Classic Generalized Consensus}

The Fast GC algorithm can process a proposal in two message delays when there is no contention on non-commuting commands. However, if there is
repeated contention on non-commuting commands, the algorithm may not make progress despite changing round, as the acceptors of the each new round
may be initialized with incompatible histories. The Fast Paxos algorithm \cite{Lamport06FastPaxos} is able to make progress under high contention by
designating a leader server that will totally order the proposals. However, to make our Fast GC algorithm work like Fast Paxos, we would have to modify it substantially. If we had spent effort testing Fast GC implementations and proving it correct, this effort would be wasted and we would have to redo all the tests and proofs from scratch.

Instead, we will now show how to obtain an algorithm with performance characteristics close to Fast Paxos, employing a leader to deal with contention, without modifying at all our existing Fast GC algorithm.

\section{Abortable Chain Agreement}
\label{sec:aca}

We now consider implementing Chain Agreement as a sequence of \emph{Abortable Chain
Agreement rounds}, where each round makes progress ordering requests or aborts
its execution and passes the baton to the next round in the sequence.
An algorithm composed of a sequence of CGC instances is called an \emph{ACA
algorithm}.

On top of the traditional roles of proposer, acceptor, and learner, ACA
introduces the new role of \emph{switcher} and a new component, the
\emph{scheduling policy}. In ACA, the assignment of the roles of proposer,
acceptor, learner, and switcher can change in each rounds, allowing for
reconfiguration. 
In normal circumstances, the proposers, acceptors,
and learners of a CGC instance work together to implement Chain Agreement.
However, if the environment becomes unfavorable to a running CGC instance, the
scheduling policy can instruct the round to abort its execution and pass the
baton, using the switchers, to a new round.

A switcher of an aborting round determines, from the state of the
acceptors, an \emph{abort sequence} that it transfers to one or several
switchers of the new round.  Upon receiving an abort sequence, a switcher of the
new round initializes the acceptors of the new round, which can then
proceed implementing SMR, until the new round itself aborts and the
process is repeated. 
We assume that in the first round, which has no previous round, every 
switchers receives the empty abort sequence.

The scheduling policy determines when to abort a CGC instance and which new ACA
round should take over. The scheduling policy must inform the switchers about the
identity of the next round and must guarantee that all switchers receive the
same information.  In this paper we abstract over the scheduling policy, assuming
that a round that aborts passes the baton to a nondeterministically chosen new
round. A reliable scheduling-policy component can realistically be implemented
as a replicated state machine running on a large number of servers otherwise
dedicated to other tasks
\cite{LamportMalkhiZhou09VerticalPaxosPrimarybackupReplication}. We will see
 in \cref{sec:rounds} that, as in Vertical Paxos, a reliable scheduling policy component allows
tolerating $f$ faults with $f+1$ replicas, while maintaining the long-term
resilience of the service by replacing faulty servers upon round change.

The clients of an ACA algorithm can send their commands to the proposers of any round (in
practice, clients send their commands to proposers of the largest
active round) and servers can use sequences learned in any round
to determine outputs.

ACA specifies the allowed behaviors of the proposers, learners, and switchers
\emph{of a single round}. Similarly to
Chain Agreement, we use the state variables $crashed$, $proposed$,
and, for every learner $l$, $learned\left[ l \right]$.  Additionally,
we use the state variables $inits$, containing the abort sequences
received from the previous round, and $aborts$, containing the abort
sequences transfered to the next round.
Two consecutive CGC instances are composed by letting the $aborts$ variable of the
first round be the $inits$ variable of the second round.

Initially, $crashed$, $proposed$, for every
learner $l$, $learned\left[ l \right]$, $inits$, and $aborts$ are all empty sets.
The next-state relation consists of five types of transitions. The
first three, $Crash\left( x \right)$, $Propose\left( l, c
\right)$, and $Learn\left( l, cs \right)$, for a learner, proposer, or
switcher $x$, for a learner $l$, and for a sequence of commands
$cs$, have the same effect as in Chain Agreement.
ACA introduces two new transitions, $Init\left(
s, cs \right)$ and $Abort\left( s, cs \right)$, for a switcher $s$ and a sequence of
commands $cs$. The transition $Init\left( s, cs \right)$ adds $cs$ to the set $inits$, and models a switcher receiving the
abort sequence $cs$ from the preceding round.
The transition $Abort\left( s, cs \right)$ adds $cs$ to the set $aborts$ and models a switcher transferring the abort sequence $cs$ to a switcher of the next round.
All the transition are guarded by the following conditions:
\begin{compactitem}
    \item[\textbf{Agreement}:] The set $\aset{learned'\left[ l
      \right] : l \in Learners}$ is compatible.
    \item[\textbf{Validity}:] Every abort history or learned history 
        is of the form $h \star cs$, where $cs$ contains only commands
        of the set $proposed$ and $h$ is a history belonging to $inits$.
    \item[\textbf{Initialization}:] If $inits$ is empty, then no
        sequence can be learned and no switcher can abort.
    \item[\textbf{Irrevocability}:] In the case of a $Learn\left( l,h
        \right)$ transition, the new learned history $h$ is an 
        extension of $learned\left[ l \right]$.
    \item[\textbf{Safe Abort}:] Every abort sequence is an extension in the 
        partial order $\leq$ of a learned history.
\end{compactitem}
Note that Agreement and Irrevocability are the same as in Chain
Agreement.

Finally, for every learner $l$, we add a weak fairness condition on the
following state transition: 
\begin{equation}
    \exists  s \in Switchers, cs \in Sequence\left( C \right) : Learn\left( l, cs \right) \vee Abort\left( s,cs \right)
\end{equation}
The weak fairness conditions ensure that when a correct learner has not
learned a sequence containing all commands proposed in the round, then
it eventually learns a new command or, if there is at least one correct switcher, at least one switch value is transfered to the next round. 
Therefore, if a client has submitted a command to a proposer, eventually,
either the client gets a response from the current round, or
the next round is initialized with an abort sequence and the
command can be resubmitted to the next round.

\subsection{The Composition Theorem}

Consider the succession of $n$ rounds numbered $1$ to $n$.  To
distinguish the variables of each round, we annotate the local
variables of each round with the number of the round. For example, the
variables of the seventh round are written $proposed_7$, for every
learner $l$, $learned_7\left[ l \right]$, $inits_7$, and $aborts_7$.

Moreover, we assume that a command, which is proposed to a round and that
is not learned before the first abort sequence is transfered to the
next round, is proposed anew to the next round.

Define the state variable $proposed$ as the union over all rounds $i$
of the $proposed_i$ variables and, for every learner $l$, 
define the $learned\left[ l \right]$ variable as the union over all
rounds $i$ of the $learned_i\left[ l \right]$ variables.

\begin{thm}[Composition Theorem]
    \label{thm:comp}
    In every execution of a succession of CGC instances, the sequence of valuations
    of the variable $proposed$ and, for every learner $l$, of the variables
    $learned\left[ l \right]$ satisfies Chain Agreement.
\end{thm}
\Cref{thm:comp} can be shown by induction on the number of rounds,
using the following lemma.
\begin{lem}
  \label{lem:comp}
    Consider two rounds numbered $i$ and $i+1$. In every execution of a
    succession of CGC instances, the sequence of valuations of the variables $inits
    = inits_i$, $propose = proposed_i \cup propose_{i+1}$, for every learner
    $l$, $learned\left[ l \right] = learned_i\left[ l \right] \cup
    learned_{i+1}\left[ l \right]$and $aborts = aborts_{i+1}$ satisfies ACA.
\end{lem}
\begin{proof}
  A mechanically-checked proof in Isabelle/HOL appears in \cref{sec:isaproofs}.
  The proofs uses a version of the specification of ACA that has a more
  executable form that presented in this section. Moreover,
  instead of sequences of commands, the Isabelle/HOL specification uses the more
  general notion of command history, presented in the next subsection.
  The Isabelle/HOL formalization of ACA uses the theory of I/O automata, which
  is also formalized in the Isabelle/HOL. The proof uses a refinement
  mapping from the composition of two consecutive ACA instances to a single ACA
  instance.
\end{proof}

\Cref{thm:comp} guarantees that the successive composition of any
number of CGC instances implements Chain Agreement.
Therefore, with ACA, one can easily build replicated services that
dynamically adapt to the current operating conditions of the system: a
replicated service has several types of rounds at its disposal, each being
tailored to particular conditions. Each time the conditions change,
the current CGC instance aborts and passes the baton to a more 
appropriate type of round, as determined by the scheduling policy. Moreover, a new type
of CGC instance can always be devised after the system is deployed and added on the fly to
the existing types of rounds.

We now describe two important practical optimizations of ACA.

\subsection{Optimizing the Execution of Read-Only Commands}
\label{sec:readonly}

Chain Agreement can be further optimized by treating 
read-only commands specially, as in Generalized Lattice Agreement
\cite{FalerioETAL12GeneralizedLatticeAgreement}.

We consider the case in which a set of servers each play the four
roles of proposer, acceptor, learner, and switcher.
Clients sends their commands to any server, which proposes them as a
proposer, and records the command as pending.
%When a server learns a history that contains one of its pending
%commands, it uses the history to determine the response to the pending
%command and forwards the response to the appropriate client.  

As observed in \cite{FalerioETAL12GeneralizedLatticeAgreement}, read-only
commands can be executed faster by directly reading from the last learned
history, instead of proposing the read commands and waiting for it to appear in
a future learned history.  Because learned histories form a chain, all the reads
return consistent values and this approach yields a \emph{serializable}
implementation of the service.

However linearizability is not satisfied if reads are served directly
from the last learned history: a lagging replica server could respond
to a read using a history which does not contain all completed update
commands. To make the executions linearizable without resorting to
proposing read-only commands as normal commands, we can employ the same
technique as in \cite{FalerioETAL12GeneralizedLatticeAgreement}: upon
receiving a read command, a server proposes a special no-op command
that commutes with all other commands; the server then responds to the
read command using the first learned history containing the no-op
command. This ensures that all the commands that had completed before
the read command was issued are present in the history.  Moreover,
since no-op commands commute with all other commands, they can be
treated more efficiently than regular read commands by the acceptors,
like, for example, in a fast round of Fast Paxos.

\section{Fast and Conservative Rounds}
\label{sec:rounds}

Most existing SMR algorithms can easily be obtained as the
composition of one or more CGC instances. 

The FLP impossibility result \cite{FischerLynchPaterson83ImpossibilityDistributedConsensusOneFaultyProcess}
implies that no algorithm can implement SMR to the letter in practice.
Instead, SMR algorithms guarantee safety (i.e. replica execute the
same sequence of commands) but may not be able to make progress (i.e.
order new commands). To make progress as likely as possible, most SMR
algorithms are structured in consecutive rounds (also called ballots,
views, or epochs). In each round, the algorithm tries to make progress
but cannot be guaranteed to succeed. When the algorithm detects that
progress is no more possible, a new round is started.  The new round
is initialized so as to preserve safety, usually by one of the
acceptors. For example, in Paxos, the leader of a newly started ballot
numbered $i$ determines a value that is \emph{safe at $i$} and
proposes it as first value to the other acceptors of the ballot $i$.
In existing SMR algorithms, new rounds are initialized ad-hoc, which
prevents mixing rounds of two different algorithms.  Instead,
ACA is an abstraction of rounds which establishes a common interface and contract for switching from
one round to the next, enabling switching from any CGC instance to any other.

To simplify the task of expressing existing algorithms as the composition of one
or more CGC instances, we propose two specifications, \emph{conservative ACA
rounds} and
\emph{fast CGC instances}, which can easily be concretized to obtain executable rounds.
We show that ACA versions of Classic Paxos, Multi-Coordinated Paxos, Chain
Replication, and Ring Paxos can be obtained by concretizing the specification of
conservative CGC instances, and that ACA versions of the fast rounds of Fast Paxos can be obtained by concretizing the specification of fast CGC instances.

Fast and conservative CGC instances are two specifications, at an
intermediate level of abstraction, which satisfy ACA. Both
specifications specify how the state of the acceptors should be
updated but abstract over the particular communication topology used
to implement the state accesses: the specifications use a global state
which is accessible by all servers in all roles. One can refine fast
or conservative rounds by implementing the state accesses and coordination
using the network, obtaining a concrete algorithm.

Conservative and fast CGC instances are specified using the state variables $inits$, $proposed$, for every learner $l$, $learned\left[ l \right]$
and $aborts$, as in ACA, and, for every acceptor $a$, two new variables
$status\left[ a \right]$, which can be either ``idle'', ``ready'', or
``stopped'' and $hist\left[ a \right]$, the local history of the
acceptor $a$, which can be either a history or the special value $None$.
Initially, $inits$, $aborts$, and, for every learner
$l$, $learned\left[ l \right]$ are empty sets, and, for every acceptor $a$,
$status\left[ a \right]$  is ``idle'' and $hist\left[ a \right]$ is the special
value $None$.

The next-state relations of conservative and fast rounds are composed
of the following types of transitions (and the default $Crash\left( x
\right)$ transition).
\begin{compactitem}
    \item A $Propose\left( p,c \right)$ transition, for a proposer $p$
        and a command $c$, not guarded, inserts $c$ in the set
        $proposed$. 
    \item An $Init\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, not guarded, inserts $h$ in the set $inits$.
    \item A $WakeUp\left( a \right)$ transition, for an acceptor $a$,
        is enabled when $status\left[ a \right]$ is ``idle''. Its
        effect is to set $hist\left[ a \right]$ to a history found in
        $inits$ and to set $status\left[ a \right]$ to ``ready''.
    \item An $Accept\left( a \right)$ transition, for an acceptor $a$,
        is enabled when $status\left[ a \right]$ is ``ready'', and
        updates $hist\left[ a \right]$ to $hist\left[ a \right]\star
        c$, where $c$ is a proposed command. In conservative rounds,
        the transition is additionally guarded by the condition that
        the resulting set of local acceptor histories,
        $\aset{hist'\left[ a \right] : a \in Acceptors}$, forms a chain
        in the partial order $\leq$ (this is typically implemented with a
        leader).
    \item A $Learn\left( l, h \right)$ transition, for a learner $l$
      and a history $h$, enabled when $h > learned\left[ l \right]$ and is
      smaller than the GLB of the histories of a learn quorum of acceptors. The effect of
        the transition is to set $learned\left[ l \right]$ to $h$.
    \item A $Stop\left( a \right)$ transition, for an acceptor $a$,
        enabled when $status\left[ a \right]$ is ``ready'', sets
        $status\left[ a \right]$ to ``stopped'' (note that
        $status\left[ a \right] = $ ``stopped'' prevents any further
        ``accept'' transition of acceptor $a$).
%    \item REALLY NEEDED? A $TimeOut\left( a \right)$ transition, for an acceptor $a$,
%        is enabled when $status\left[ a \right]$ is ``idle'' and sets 
%        $status\left[ a \right]$ to ``stopped'' and $hist\left[ a
%        \right]$ to the special value $None$. 
    \item An $Abort\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, is enabled when there exists a abort quorum
        $R$ such that, for every member $a$ of $R$, $status\left[ a
        \right]$ is ``stopped'' and $h \in SafeAborts\left( R
        \right)$. Its effect is to insert  $h$ in the set $aborts$. 
\end{compactitem}

On top of the additional guard on the $Accept$ transition present only
in conservative rounds, conservative and fast CGC instances differ in
their definition of learn quorums, abort quorums, and in their
definition of the $SafeAborts\left( R \right)$ operator.
To concisely define read and learn quorums, we define 
\begin{equation}
    Q\left( f \right) = \aset{as \subseteq Acceptors :
        Card\left(as\right) \geq \left\lfloor f\times Card\left(
    Acceptors \right)\right\rfloor + 1}.
\end{equation}

In conservative CGC instances, abort and learn quorums are defined such
that the intersection between two learn quorums is nonempty and the
intersection between a learn quorum and an abort quorum is nonempty.
The definition leads to two interesting cases: $LearnQ = AbortQ =
Q\left( 1/2 \right)$; $LearnQ = \aset{Acceptors}$ and $AbortQ =
\aset{\aset{a} : a \in Acceptors}$. The operator $SafeAborts\left( R
\right)$ is defined as the singleton set containing the maximum
element over the local histories of the acceptors in $R$ which are not
$None$. Note that the maximum exists because conservative rounds
guarantee that the histories of the acceptors form a chain.

In fast CGC instances, abort and learn quorums are defined such that
the intersection between any two learn quorums is nonempty and the
intersection between any two learn quorums and an abort quorum is
nonempty.  The definition leads to three interesting cases: $LearnQ =
AbortQ = Q\left( 2/3 \right)$; $LearnQ = Q\left( 1/2 \right)$ and
$AbortQ = Q\left( 3/4 \right)$; $LearnQ = \aset{Acceptors}$ and
$AbortQ = \aset{\aset{a : a \in Acceptors}}$.
The operator $SafeAborts\left( R \right)$ is defined as the set of
maximal elements over the set $\aset{GLB\left( \aset{hist\left[ a
\right] : a \in R \cap L \wedge hist\left[ a \right]\neq None} \right)
: L \in LearnQ}$.

The cases in which abort quorums are singleton sets, in both
conservative and fast rounds, allow tolerating
$f$ faults with $f+1$ servers: a round can abort as long as one
acceptor is correct and a new round, possibly using new correct
servers, can take over. Note that changing round cannot happen
without the scheduling policy instructing the switchers about the
identity of the next round, and thus tolerating $f$ faults with
$f+1$ servers relies on the assumption of a reliable scheduling policy.

Define, for every learner $l$, the history variable $\overline{learned}\left[ l \right]$ which is initialized to the empty set and which is
updated each time a new local history is accepted by the learner $l$ by
inserting the new local history in $\overline{learned}\left[ l \right]$ (see \cite{AbadiLamport91ExistenceRefinementMappings} for the
definition of a history variable).
\begin{thm}
  \label{thm:rounds}
    Both conservative and fast CGC instances implement ACA under the refinement
    mapping that consists in substituting $\overline{learned\left[ l \right]}$
    for $learned\left[ l \right]$.
\end{thm}

\section{Examples of Concrete ACA Algorithms}
\label{sec:examples}

In this section we show how to modify the rounds of Classic Paxos,
Multi-Coordinated Paxos, Chain Replication, Ring Paxos, and Fast Paxos to obtain
CGC instances. \Cref{thm:comp} then guarantee that all those different types of
rounds can be used in the same algorithm to yield an implementation of Chain
Agreement.

\subsection{Classic Paxos}

The rounds of the Classic Paxos algorithm can be modified so as to
obtain an implementation of conservative CGC instances.  We define
Classic Paxos CGC instances by specializing the specification of
conservative CGC instances, giving a concrete protocol for disseminating
the proposed commands to the acceptors and enforcing that the learned
histories form a chain.

Each Classic Paxos round has a unique leader among the acceptors which
centrally orders the proposed commands, which are accepted by the
other acceptors in the order determined by the leader.  This protocol
ensures that every local acceptor history is a prefix of the local
history of the leader, therefore guaranteeing that the local acceptor histories
forming a chain.

Acceptor broadcast each newly accepted history to the learners, which
learn a history $h$ when they receive enough extensions of $h$ from the
acceptors.  When instructed by the scheduling policy to abort, a
switcher sends a stop message to the acceptors, which irrevocably stop
updating their local history and send back to the switcher an
acknowledgement containing their local history. A switcher
determines an abort history once it has received acknowledgments from
an abort quorum of acceptors $R$, using the operator $SafeAborts\left(
R \right)$.

The specification of Classic Paxos CGC instances uses the variables of
the specification of conservative CGC instances, namely the variables
$proposed$, $inits$, $aborts$, the arrays $learned\left[ l \right]$,
$status\left[ a \right]$, and $hist\left[ a \right]$, and two
additional arrays $accepted\left[ l \right]\left[ a \right]$ and
$stop\left[ s \right]\left[ a \right]$, where $l$ is a learner, $a$ is
an acceptor, and $s$ is a switcher.

There is also a network component that we do not describe explicitly.
However, the network allows any servers to communicate by sending and
receiving messages.

Initially, the variables $proposed$, $inits$, $aborts$, and, for every
learner $l$, $learned\left[ l \right]$ are empty sets. For every
acceptor $a$, $status\left[ a \right]$ is ``idle'' and $hist\left[ a
\right]$ is the special value $None$. For every learner $l$ and acceptor $a$,
$accepted\left[ l \right]\left[ a \right]$ is the special value
$None$. For every switcher $s$ and acceptor $a$, $stopped\left[ s
\right]\left[ a \right]$ is set to $false$.  There are initially no
messages in the network.

The next-state relation is composed of the following types of
transitions, where the definitions of learn quorums, abort quorums,
and of the $SafeAborts\left( R \right)$ operator are the ones of conservative
CGC instances.
\begin{compactitem}
    \item A $Propose\left( p,c \right)$ transition, for a proposer $p$
        and a command $c$, not guarded, inserts $c$ in the set
        $proposed$ and sends the message $\aseq{\quo{prop},c}$ to
        the leader.
    \item An $Init\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, not guarded, inserts $h$ in the set $inits$ and
        sends the message $\aseq{\quo{init},h}$ to the leader.
    \item A $WakeUp\left( leader \right)$ transition, enabled when
        $status\left[ leader \right]$ is ``idle'' and the leader can
        receive a $\aseq{\quo{init},h}$ message. Its effect is to
        receive the message, set $hist\left[ leader \right]$ to $h$,
        to set $status\left[ leader \right]$ to ``ready'', and to
        broadcast the message $\aseq{\quo{leader-accept},h}$ to all
        other acceptors.
    \item An $Accept\left( leader \right)$ transition, enabled when
        $status\left[ leader \right]$ is ``ready'' and the leader can
        receive a $\aseq{\quo{prop},c}$ message. The effect of the
        action is to receive the message, to update $hist\left[ leader
        \right]$ to $hist\left[ leader \right]\star c$, and to
        broadcast the message $\aseq{\quo{ack},hist\left[ leader
        \right]\star c}$ to the learners, and to broadcast the message
        $\aseq{\quo{leader-accept},hist\left[ leader \right]\star c}$
        to all the other acceptors.
    \item An $Accept\left( a \right)$ transition, for an acceptor $a$
        which is not the leader, is enabled when $status\left[ a
        \right]$ is ``ready'' and $a$ can receive a
        $\aseq{\quo{leader-accept},h}$ message where $h > hist\left[ a
        \right]$. The effect of the action is to receive the message,
        to update $hist\left[ a \right]$ to $h$, and to broadcast the
        message $\aseq{\quo{ack},hist\left[ a \right]\star c}$ to the
        learners.
    \item A $RcvAck\left( l \right)$ transition, for a learner $l$, is
        enabled when $l$ can receive a message $\aseq{\quo{ack},h}$
        from an acceptor $a$. Its effect is to receive the message and
        to set $accepted\left[ l \right]\left[ a \right]$ to $h$.
    \item A $Learn\left( l, h \right)$ transition, for a learner $l$
        and a history $h$, is enabled when $h > Max\left(
        learned\left[ l \right] \right)$ and there is a learn quorum
        $Q$ of acceptors such that $h = GLB\left( \aset{accepted\left[
        l \right]\left[ a \right] : a \in Q }\right)$. Its effect is
        to insert $h$ in $learned\left[ l \right]$.
    \item A $Stop\left( s \right)$ transition, for a switcher $s$,
        always enabled, broadcasts a $\aseq{\quo{stop}}$ message to all
        the acceptors.  This transition models a switcher being
        instructed by the scheduling policy to abort the current round.
    \item A $Stop\left( a \right)$ transition, for an acceptor $a$,
        enabled when $a$ can receive a $\aseq{\quo{stop}}$ message
        from a switcher $s$. Its effect is to receive the message, set
        $status\left[ a \right]$ to ``stopped'', and to send the
        message $\aseq{\quo{stopped},hist\left[ a \right]}$ to the
        switcher $s$.
    \item A $RcvStopped\left( s \right)$ transition, for a switcher
        $s$, enabled when $s$ can receive a $\aseq{\quo{stopped},h}$
        message from an acceptor $a$. Its effect is to receive the
        message and to set $stopped\left[ s \right]\left[ a \right]$
        to $h$.
    \item An $Abort\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, is enabled when there exists an abort quorum
        $R$ such that, for every member $a$ of $R$, $stopped\left[ s
        \right]\left[ a \right]$ is not $false$ and $h \in
        SafeAborts\left( R \right)$, where $SafeAborts\left( R
        \right)$ is computed using the array $stopped\left[ a
        \right]$. Its effect is to insert  $h$ in the set $aborts$. 
\end{compactitem}
Compared to the rounds of the original Classic Paxos, Classic Paxos ACA
rounds add an extra level of indirection when changing round: in the
original rounds, the leader of the new round directly queries the
acceptors of the preceeding round, whereas in CGC instances, the
switchers query the acceptors and then send their abort histories to
the leader. However, changing round is an infrequent event, therefore
this difference should not have much practical impact on performance.
\begin{thm}
  Classic Paxos CGC instances satisfy Abortable Chain Agreement.
\end{thm}
\begin{proof}
  A Classic Paxos CGC instance implements the specification of Abortable
  Chain Agreement under the refinement mapping consisting in
  projecting the state of the round onto the variables $proposed$,
  $inits$, $aborts$, and, for every learner $l$, $learned\left[ l
  \right]$.
\end{proof}

\subsection{Multi-Coordinated Paxos, Chain Replication, and Ring Paxos.}

Multi-Coordinated Paxos, Chain Replication, and Ring Paxos rounds have a similar
structure as Classic Paxos rounds but differ in the way the leader is
implemented and in the way that the histories of the leader are disseminated to
the other acceptors.

Multi-Coordinated Paxos avoids depending on a unique leader by using a
distributed leader implementation that can make progress as long as a
strict majority of the acceptors are correct, trading off resilience
for a higher time complexity. Conservative ACA
rounds can be implemented like with Classic Paxos, replacing the
leader by the distributed leader of Multi-Coordinated Paxos.

In Chain Replication, acceptors are arranged in a chain whose head is
the leader of the round. As in Classic Paxos, abort histories and
proposed commands are sent to the leader. However, instead of the
leader directly sending its local history to all the other acceptors,
the local history of the leader is
forwarded along the chain from one acceptor to the other until it
reaches the last acceptor, which sends the history to the learners. 
By spreading the load more evenly on the all the servers, Chain Replication
can achieve higher throughput that Classic Paxos.
Conservative CGC instances can be implemented like with Classic Paxos,
replacing the broadcast of the leader by the propagation of the leader's history
along the chain of servers.

Ring Paxos optimizes Chain Replication by having the leader broadcast
commands to the acceptors using IP multicast and only sending hashes
of the commands along the chain. Ring Paxos round can be modified to implement
conservative CGC instances in a similar way as for Chain Replication.

\subsection{Fast Paxos}

Fast Paxos is composed of two types of rounds: classic rounds, like in Classic
Paxos, and fast rounds.
The fast rounds can be transformed into
implementations of fast CGC instances. Instead of sending their
commands to a leader, as in Classic Paxos, the proposers of fast
rounds directly broadcast their commands to all the acceptors. As long
as commands are received in the same order by a learn quorum of
acceptors or are received in different orders but commute, commands can be learned in two message-propagation delays instead of three in Classic Paxos.
However, if non-commuting commands are received in different orders by
several acceptors, the local history of the acceptors may not form a
chain anymore and the rounds must abort. The fast rounds of Fast
Paxos implement fast CGC instances.

Fast Paxos CGC instances are specified using the same set of variables as for
Classic Paxos, initialized to the same values.
The next-state relation is composed of the following types of
transitions, where the definitions of learn quorums, abort quorums,
and of the $SafeAborts\left( R \right)$ operator are the ones of fast
CGC instances.
\begin{compactitem}
    \item A $Propose\left( p,c \right)$ transition, for a proposer $p$
        and a command $c$, not guarded, inserts $c$ in the set
        $proposed$ and broadcasts the message $\aseq{\quo{prop},c}$ to
        all the acceptors.
    \item An $Init\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, not guarded, inserts $h$ in the set $inits$ and
        broadcasts the message $\aseq{\quo{init},h}$ to all the acceptors.
    \item A $WakeUp\left( a \right)$ transition, for an acceptor $a$,
        is enabled when $status\left[ a \right]$ is ``idle'' and $a$
        can receive an $\aseq{\quo{init},h}$ message. Its effect is to
        receive the message, set $hist\left[ a \right]$ to $h$, and to
        set $status\left[ a \right]$ to ``ready''.
    \item An $Accept\left( a \right)$ transition, for an acceptor $a$,
        is enabled when $status\left[ a \right]$ is ``ready'' and $a$
        can receive a $\aseq{\quo{prop},c}$ message. The effect of the
        action is to receive the message, to update $hist\left[ a
        \right]$ to $hist\left[ a \right]\star c$, and to broadcast
        the message $\aseq{\quo{ack},hist\left[ a \right]\star c}$ to
        the learners.
    \item The $RcvAck\left( l \right)$ transitions, for a learner $l$,
        the $Learn\left( l, h \right)$ transitions, for a learner $l$
        and a history $h$, the $Stop\left( s \right)$ transitions, for
        a switcher $s$, $Stop\left( a \right)$ transitions, for an
        acceptor $a$, the $RcvStopped\left( s \right)$ transitions,
        for a switcher $s$, and the $Abort\left( s, h \right)$
        transitions, for a switcher $s$ and a history $h$, which are
        all the same as in Classic Paxos CGC instances.
\end{compactitem}

As for Classic Paxos CGC instances, Fast Paxos CGC instances implement ACA
under the refinement mapping consisting projecting the state of the
round onto the variables $proposed$, $inits$, $aborts$, and, for every
learner $l$, $learned\left[ l \right]$.


\newpage

\printbibliography


\begin{appendix}
  
  \label{sec:isaproofs}
 
%  \includepdf[pages={1},pagecommand={\section{Mechanical Proof of the
%  Composition Theorem in Isabelle/HOL}\label{sec:isaproofs}},scale=1]{../IsabellePDFS/podctheories.pdf}
%  \includepdf[pages={2-},pagecommand={},scale=1]{../IsabellePDFS/podctheories.pdf}
  
\end{appendix}

\end{document}
