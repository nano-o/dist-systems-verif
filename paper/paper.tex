\input{header.tex}
\input{defs.tex}

\addbibresource{managed.bib}

\title{Composable Generalized Consensus}

\author{
  Rachid Guerraoui\\
  \texttt{rachid.guerraoui@epfl.ch}
  \and
  Giuliano Losa\\
  \texttt{giuliano@losa.fr}
}

\date{}

\begin{document}

\maketitle

\input{abstract3}

\vspace{2cm}
\begin{center}
This is a regular paper.
\end{center}

\newpage

\input{intro}

\input{roadmap}


\section{Related Work}
\label{sec:related}

SMR is often implemented using a sequence of instances of Consensus.
There are numerous examples of consensus algorithms in the literature.
However, Composable Generalized Consensus is based on Generalized Consensus. 
In Generalized Consensus \cite{Lamport05GeneralizeConsensus}, the learners of a
SMR algorithm repeatedly learn new values belonging to a \emph{C-struct set}. A
C-struct can be seen as a data structure encoding  sequences of commands modulo
the ordering of commands that commute. The advantage of Generalized
Consensus over Consensus is that
two servers which receive commuting commands in different orders can still reach
agreement on C-structs without further synchronization. 
Our Composable Generalized Consensus can also use C-structs, with the same advantages
as Generalized Consensus. For simplicity, the presentation in this
paper uses the more concrete notion of \emph{command history} (see
\cref{sec:histories}).

Generalized Lattice Agreement \cite{FalerioETAL12GeneralizedLatticeAgreement} allows more efficient implementations of SMR when
commands are partitioned into read-only commands and write-only commands.
Composable Generalized Consensus supports the same optimizations as Generalized Lattice Agreement.
Notably, read-only commands are treated specially so as to optimize their
executions (see \cref{sec:readonly}).

Vertical Paxos algorithms 
\cite{LamportMalkhiZhou09VerticalPaxosPrimarybackupReplication} are SMR algorithms
whose set of replica servers can be reconfigured without stopping to process new
commands. Crashed servers can therefore be replaced by new servers, ensuring the
long-term availability of a replicated service.
Vertical Paxos algorithms use an external reconfiguration master, itself
implemented as a replicated state machine, which determines the servers that should
be removed from or added to the configuration.
CGC shares some aspects of Vertical Paxos algorithms: the task of the
reconfiguration master is similar to the task of the scheduling policy
component of CGC (see \cref{sec:aca}).
However, in CGC, changing round can be used not only to replace crashed servers
but also to change the algorithm that the servers are running. 
Changing algorithm is not possible in Vertical Paxos.
In both Vertical Paxos and CGC, a reliable reconfiguration master, resp. a
reliable scheduling policy, allows tolerating $f$ crashes with $f+1$ servers.

The Abstract Framework \cite{GuerraouiETAL10Next700BftProtocols} proposes to
build Byzantine Fault-Tolerant algorithms as the succession of abortable rounds
called Abstract instances. Abstract instances must satisfy the Abstract
correctness properties. By construction, the composition of any number of
Abstract instances is a correct SMR implementation. However, the Abstract
Framework uses totally ordered sequence of commands, making difficult the
optimization of the execution of commuting and read-only commands that are
possible with CGC.  Moreover, the Abstract Framework does not identify the
crucial role of the scheduling policy for the long-term resilience of a service.
Composable Generalized Consensus combines the ideas of the Abstract Framework,
Generalized Paxos, Generalized Lattice Agreement, and Vertical Paxos in a
unified abstraction. 

Finally, the Speculative Linearizability framework
\cite{GuerraouiKuncakLosa12SpeculativeLinearizability} was a first
attempt by the authors of the present paper at generalizing the
Abstract Framework and mechanically proving its properties with the
Isabelle/HOL interactive
proofs assistant.  Like the Abstract Framework, the
Speculative Linearizability framework is based on totally ordered
sequences of commands, with the same drawback for the execution of
commuting and read-only commands. Moreover, Composable Generalized Consensus is both
conceptually simpler and subsumes the Speculative Linearizability
property.  Like Speculative Linearizability, Composable Generalized Consensus
and its main property, the composition theorem (\cref{thm:comp}), are
formalized in Isabelle/HOL (see \cref{sec:isaproofs}).

Composable Generalized Consensus is the most complete framework to date for building
adaptive replication algorithms out of heterogeneous rounds, making state of the
art optimization techniques available to algorithm designers, which was not
possible before.

\section{Generalized Consensus}
\label{sec:gc}

We consider a set of asynchronous servers communicating by message passing in
an asynchronous network. Servers are not necessarily sequential processes.
We describe a behavior of the system using an infinite sequence of states,
where a state is a function from variable names to values (i.e. a valuation of
variables). We specify sets of allowed behaviors using an initial predicate
and a next-state relation expressed using unprimed and primed variables in the
style of TLA+ \cite{Lamport02SpecifyingSystems}. However, for succinctness, our
specification are rather informal, eluding non-essential details.

Our specifications all contain by default a state variable $crashed$ and
 a $Crash\left( x \right)$ transition which is always enabled. A server $x$ faithfully
executes the algorithm that is assigned to it until its identifier is added to
the variable $crashed$ by a $Crash\left( x \right)$ transitions.  When the identifier of a
server is in the set $crashed$, all its actions are disabled.  In an execution,
a server that is never added to the set $crashed$ is said correct.

We consider a computing service exposing a set of commands $C$. The
service has a sequential specification consisting of an initial state
and a deterministic transition relation in which every command
atomically updates the state of the service and produces an output. We
assume that duplicate commands have no effect on the state of the
service and return the same output as did their first occurrence.

The goal of SMR is to provide a reliable implementation of the computing service despite server crashes.
To do so, an SMR algorithm replicates the execution of the service on several servers. 
In a traditional SMR algorithm, the servers can be seen has playing one or
several of three roles: \emph{proposers}, which propose new commands to execute,
\emph{acceptors}, whose role is to order commands, and
\emph{learners}, which learn about the growing sequence of commands to
execute \cite{lamport2001paxos} by querying the acceptors. Often, but not necessarily, every replica server plays the
three roles of proposer, acceptor, and learner: clients of the service
send their commands to some replica servers; replica servers submit
the received commands to the SMR protocol in the role of proposers;
replica servers respond to commands by executing the commands learned
in the role of learners; replica servers participate in command
ordering in the role of acceptors.  A SMR algorithm guarantees that
the replicas learn the same sequences of commands, possibly up to the
reordering of commuting commands. Therefore, every learner can respond to client
requests using its last learned sequence of commands, and clients will receive
the same responses as the ones produces by a centralized, non-replicated,
service. 

\subsection{Histories}

To simplify the specification of SMR algorithms optimizing the execution of commuting commands 
we will use the notion of a command history, or history for short, in our specification of Composable Generalized Consensus.

A command history, abbreviated history, is a data structure that
represents a set of sequences which are the same modulo the reordering
of commuting commands and addition or removal of duplicate commands.
We say that two commands $c_1$ and $c_2$ \emph{commute} when, in every
sequential execution of the service in which $c_1$ and $c_2$ are adjacent,
reversing the order of $c_1$ and $c_2$ does not change any of the outputs,
including those given to $c_1$ and $c_2$.

All the sequences represented by a given history contain the same commands
and determine the same outputs for each command. Therefore, to implement a
replicated service, it is sufficient that learners learn histories instead of
sequences. For a thourough discussion of command histories we refer the reader
to \cite{Lamport05GeneralizeConsensus}. Our Isabelle/HOL formalization \cite{}
abstracts over the concrete representation of histories, assuming only their
necessary properties. Command histories were first introduced in trace theory
\cite{Mazurkiewicz84Semantics}.

Command histories can be built from the empty history, noted $\bot$, using the $\star$
operator, which appends a sequence of commands $cs$ to a history $h$, obtaining
the history $h \star cs$. We can then define define an ordering relation among
histories: $h_1 \leq h_2$ when there exists a sequence $cs$ such that $h_2 = h_1
\star cs$. Histories have the property that every finite set $H$ of histories
has a \emph{greatest lower bound}, noted $GLB\left(H\right)$. 
We say that a history $h$ contains the command $c$ when there exists a sequence of commands 
$cs$ containing $c$ such that $\bot \star cs = h$.
Finally, we say
that a set of histories $H$ is compatible when all the histories in $H$ have a
common upper bound $h$ such that $h$ can be built using only commands found in
the histories in $H$.
%The maximum over a set of histories $H$ is denoted by $Max\left( H \right)$ and $Max\left( \aset{} \right)$ is the empty history.

The major advantage that histories have over sequences is that if a sequence
$cs_1$ is obtained by reordering the commuting commands of $cs_2$, then $h \star
cs_1 = h \star cs_2$. Therefore, in SMR implementations, servers can agree on
a history even though they chose different orders for some commuting commands.

\subsection{Generalized Consensus}

Generalized Consensus precisely specifies the allowed behaviors of the roles of \emph{proposers}
and \emph{learners}, abstracting over the behavior of acceptors. 

We specify Generalized Consensus using the variables $crashed$, the variable $proposed$, containing the proposed commands,
and, for every learner $l$,  a variable $learned\left[ l \right]$, holding a history. Initially, $crashed$ and $proposed$ are empty sets and, for every learner $l$,
$learned\left[ l \right]$ is the empty history. 
Remember that any transition involving a server $x$ is always disabled when $x\in crashed$. 

The next-state relation consists of three types of transitions:
$Crash\left( x \right)$ transitions; $Propose\left( l, c
\right)$ transitions, for a learner $l$ and a command $c$, in which
$c$ is inserted in the set $proposed$; $Learn\left( l,h \right)$
transitions, for a learner $l$ and a history $h$, in
which $learned\left[ l \right]$ is update to $h$. 
A $Learn\left( l, h \right)$ transition is guarded by the following conditions:
\begin{compactitem}
    \item[\textbf{Agreement}:] The set of learned histories after the transition, $\aset{learned'\left[ l
      \right] : l \in Learners}$, is compatible.
    \item[\textbf{Validity}:] Every learned history contains only
        proposed commands.  
    \item[\textbf{Irrevocability}:] The new learned history
      $learned'\left[ l \right]$ is an extension of $learned\left[ l
        \right]$. 
        %\\ $cs > Max\left(learned\left[ l \right]\right)$.
\end{compactitem}

When clients of a replicated service submit their commands to the proposers and learners compute the response to give to clients using their local $learned\left[ l \right]$ history, the three properties above imply that the clients see a \emph{linearizable} execution
\cite{HerlihyWing90LinearizabilityCorrectnessConditionConcurrentObjects}.

\section{Modular Implementations of Generalized Consensus}

Before formally introducing our framework, let us see how we can build a modular implementation similar to Fast Paxos by first designing a fast but fragile modular GC algorithm, then combining it unchanged with a more resilient module, obtaining an algorithm similar to Fast Paxos. 

\subsection{The Broadcast-Decide Algorithm}

In our first algorithm, Broadcast-Decide, learners each maintain a variable $hist\left[ l \right]$ which contain the currently accepted history of learner $l$.
For every learner $l$, $accepted\left[ l \right]$ is initialized to the empty history.
A proposer broadcasts its proposal to all the learners which unconditionally accept it by appending the proposal to their $accepted\left[ l \right]$
history and by broadcasting to all the acceptors their new accepted history, $accepted'\left[ l \right]$.

Each acceptor $l$ maintains a variable $learned\left[ l \right]$, initially the empty history, containing its last learned history. A learner $l$
learns a new history $h$ when $h \geq learned\left[ l \right]$ and $h$ is a lower bound of the set consisting of the maximal accepted history received
from each acceptor.

More precisely, the Broadcast-Decide algorithm is described by the transition relation obtained as the disjunction of the
following transitions. We do not model the network explicitly using state variables but instead we just say that servers send, broadcast, or receive messages.

\begin{compactitem}

\item A $Propose\left( p,c \right)$ transition, for a proposer $p$ and a command $c$, not guarded, broadcasts the message $\aseq{\quo{prop},c}$ to all
the acceptors.

\item An $Accept\left( a \right)$ transition, for an acceptor $a$, enabled when $a$ can receive a $\aseq{\quo{prop},c}$ message from a proposer,
updates $accepted\left[ a \right]$ to $accepted\left[ a \right]\star \aseq{c}$ and broadcasts the message $\aseq{\quo{accepted}, a, accepted'\left[ a \right]}$ to all the learners
($accepted'\left[ a \right]$ is the new value of the variable $accepted\left[ a \right]$).

\item A $Learn\left( l, h \right)$ transition, for a learner $l$ and a history $h$, enabled when $h \geq learned\left[ l \right]$ and $h$ is a lower
bound of the set $\aset{MaxReceivedFrom\left( a \right) : a \in Acceptors}$, where $MaxReceivedFrom\left( a \right)$ is the maximal accepted history received
from the acceptor $a$.
 Its effect is to set $learned\left[ l \right]$ to $h$.

\end{compactitem}

The algorithm Broadcast-Decide cannot learn new histories if two acceptors $a_1$ and $a_2$ have two histories which are not compatible because, since an acceptor can only append new proposals to its history $accepted\left[ l \right]$, all the possible future lower bounds of the set $\aset{MaxReceivedFrom\left( a \right) : a \in Acceptors}$ are bounded above by $GLB\left( \aset{accepted\left[ a_1 \right], accepted\left[ a_2 \right]} \right)$.
This situation occurs when non-commuting commands contend: for example, when two different proposers send two commands $c_1$ and $c_2$ that do not commute, the acceptors $a_1$ accepts $c_1$ before $c_2$, and the acceptor $a_2 \neq a_1$ accepts $c_2$ before $c_1$.

To overcome this deadlock, we will make some modifications to the algorithm that will allow us to start a fresh instance $B_2$, called a round, of the Broadcast-Decide algorithm.
To allow switching to a fresh new instance of the Broadcast-Decide algorithm, we introduce the new role of switcher, that any server can play additionally from being a proposer, acceptor, or learner.
A switcher will extract an accepted history from the first round and try to initialize all the acceptors of the second round with the extracted history.
If we are luckier than in the first round, there may be no contention between switchers and every acceptor $a$ will start accepting new proposals in the second round based on the same initial history.

We write $B_1$ for the first round of the Broadcast-Decide algorithm.
The rounds $B_1$ and $B_2$ each have their own copies of the arrays $accepted$ and $learned$, noted $accepted_1$, $accepted_2$, $learned_1$, and $learned_2$ when the round is not clear from the context. 
We assume that network messages are tagged with a round number and that a message tagged with $i\in\aset{1,2}$ can only be received in $B_i$, creating two isolated logical networks. 

In our example with the deadlock, switchers have the role of initializing the new round of $B_2$ so that it can resume the execution and overcome the deadlock.
We modify the Broadcast-Decide algorithm by introducing new transitions and modifying the $Accept\left( a \right)$ transition as follows.
We add the new variables $status\left[ a \right]$ for every acceptor $a$, which are initialized to $\quo{idle}$. The purpose of $status\left[ a \right]$ is to prevent the acceptor $a$ from accepting commands until its status changes from $\quo{idle}$ to $\quo{ready}$ and to stop it when its status goes to $\quo{stopped}$.
We also add the new arrays of variables $inits\left[ s \right]$ and $aborts\left[ s \right]$, for every switcher $s$, initialized to the empty history in $B_1$ and to $\quo{none}$ in $B_2$. The purpose of the arrays of variables $aborts$ and $inits$ is to transfer state between $B_1$ and $B_2$. The arrays of variables $aborts$ and $inits$ form what we call the \emph{inter-round} interface.
As for the arrays $accepted$ and $learned$, the rounds $B_1$ and $B_2$ each have their own copy of the arrays of variables $status$, $inits$, and $aborts$, and we adopt add subscripts denoting the round when the round is not clear from the context. 

\begin{compactitem}
\item We modify the $Accept\left( a \right)$ transition by enabling it only if $status\left[ a \right] = \quo{ready}$ and by having the acceptor $a$ additionally broadcast its new accepted value to the switchers.
Switchers detect that the algorithm is deadlocked when they have received two messages $\aseq{\quo{accepted}, a_1, h_1}$ and $\aseq{\quo{accepted}, a_2, h_2}$ such that $h_1$ and $h_2$ are incompatible.

\item The $Stop\left( s \right)$ transition, for a switcher $s$, is enabled when the switcher has detected a deadlock or has waited for too long without receiving any message from the acceptors. Its effect is to broadcast a $\aseq{\quo{stop}, s}$ message to the acceptors.

\item The $Stop\left( a \right)$ transition, for an acceptor $a$, is enabled when the acceptor $a$ can receive a $\aseq{\quo{stop}, s}$ message from a switcher $s$.
  Its effect is to update $status\left[ a \right]$ to $\quo{stopped}$ and to send the message $\aseq{\quo{stopped},a,accepted\left[ a \right]}$ to the switcher $s$.

\item The $Abort\left( s, h\right)$ transition, for a switcher $s$, is enabled when $aborts\left[ s \right] = \quo{none}$ and the switcher has received
  at least one $\aseq{\quo{stopped}, a, h}$ message. Its effect is to set the variable $aborts\left[ s \right]$ to the history $h$ received in the $\quo{stopped}$ message. The history $aborts'\left[ s \right]$ is then called an abort history. Because the switcher has received a $\aseq{\quo{stopped},a,h}$ message from the acceptor $a$, the acceptor $a$ no longer accepts new proposals, therefore no history greater than $h$ can ever be learned anymore. Therefore, an abort history can be seen as a safe starting point for a new round to resume execution. 

\item The $Init\left( s,h \right)$ transition, for a switcher $s$, is enabled when $inits\left[ s \right]=\quo{none}$ and $h$ is a history, and its effect is to set $inits\left[ s \right]$ to $h$, and to broadcast the message $\aseq{\quo{init}, h}$ to the acceptors.  The history $inits\left[ s \right]$ is then called an init history.
As described below, in $B_2$ composed with $B_1$, this history is constrained to be an abort history of $B_1$.

 \item The $WakeUp\left( a \right)$ transition, for an acceptor $a$, is enabled when $status\left[ a \right]$ is $\quo{idle}$ and there is at least one switcher $s$ such that $inits\left[ s \right] \neq \quo{none}$. Its effect is to set set $status\left[ a \right]$ to $\quo{ready}$ and $accepted\left[ a \right]$ to $inits\left[ s \right]$.

\end{compactitem}

The composition $B_1\times B_2$ of the two rounds $B_1$ and $B_2$ is obtained as follows:
\begin{itemize}
  \item We Require that a transition of the composition be a transition of either specification but not both, except for the $Abort\left( s \right)$ and $Init\left( s \right)$ transitions, which are allowed to take place at the same time. 
  \item We require that $aborts_1 = inits_2$ at all times.
\end{itemize}
The composition models two independent rounds being linked only by the switchers passing the abort histories from $B_1$ to $B_2$, where they become init histories: switchers gather abort histories in $B_1$, then they broadcast them as init histories to the acceptors in $B_2$. The acceptors then initialize their state to a received init history in the $WakeUp\left( a \right)$ transition.
This process can be repeated as often as needed, with new rounds $B_3$, $B_4$, etc., to try to make progress learning new histories.

The Broadcast-Decide algorithm can no more learn new histories whenever one acceptor crashes because the learners wait for the lower bound of the accepted histories of all acceptors to increase. However, the round-change mechanism is very resilient to faults and can abort anyway thanks to the timeout triggering the $Stop\left( s \right)$ transition: a single live acceptor with a single live switcher can abort the execution and change round.

To allow a new round to make progress after some acceptors crashed in a previous round, we introduce a component called the switching policy. With the Broadcast-Decide rounds, the task of the switching policy is to update the set of acceptors from one round to the next, ensuring that failed acceptors are removed or replaced by live ones. The switching policy must be queried by the switchers upon changing round to determine the new set of acceptors. This information can then be piggybacked to the
acceptors and learners, or the acceptors and learners can directly query the switching policy. 
The set of switchers can be changed from round to round in the same fashion. Moreover, in an algorithm composed of several different types of rounds, the switching policy will also be responsible for choosing the type of round to execute next.

\section{The Simple-Leader Algorithm} 

The Broadcast-Decide algorithm can process a proposal in two message delays when there is no contention on non-commuting commands. However, if there is
repeated contention on non-commuting commands, the algorithm may not make progress despite changing round, as the acceptors of the each new round
may be initialized with incompatible histories. The Fast Paxos algorithm \cite{Lamport06FastPaxos} is able to learn new histories under high contention by
designating a leader server that will totally order the proposals. However, to make our Broadcast-Decide algorithm work like Fast Paxos, we would have to modify it substantially. If we had spent effort testing Broadcast-Decide implementations and proving it correct, this effort would be wasted and we would have to redo all the tests and proofs from scratch.

Instead, we will now show how to obtain an algorithm with performance characteristics close to those of Fast Paxos, employing a leader to deal with contention, without modifying at all our existing Broadcast-Decide algorithm.
To do so, we define a new kind of rounds, the Simple-Leader rounds, which uses the same inter-round interface as the Broadcast-Decide algorithm. 
The inter-round interface will allow use to combine Simple-Leader rounds and Broadcast-Decide rounds in any order, obtaining an algorithm which can adapt its strategy to its environment, selecting the Broadcast-Decide algorithm under low contention and switching to the more expensive Simple-Leader algorithm when there is contention.

In the Simple-Leader algorithm, the proposers send their proposals to a distinguished acceptor called the leader. 
The leader totally orders the proposals it receives and forwards them to all the acceptors which accept them in the order defined by the leader.
As in the Broadcast-Decide algorithm, acceptors broadcast their accepted histories to the learners, which learn a new history when it is a lower bound of 
the set of $\aset{MaxReceivedFrom\left( a \right) : a \in Acceptors}$.
Thanks to its leader, a Simple-Leader round continues to learn new histories even when proposals conflict because the leader ensures that the accepted histories of the acceptors remain consistent. 

The Simple-Leader algorithm treats all proposals equally and does not take advantage of commutativity. When the leader does not crash, each proposal is processed in three message delays. In comparison, in the absence of failures, the Broadcast-Decide algorithm processes non-conflicting proposals in two message delays but can take arbitrarily long to process conflicting proposals.

To initialize a Simple-Leader round, the switchers send their init histories to the leader, which choose a unique one that it forwards to the acceptors, guaranteeing that all the acceptors are initialized with the same history even when conflicting init histories are sent concurrently.

The Simple-Leader algorithm can not learn new histories if its leader, or any other acceptor, crashes.
To recover from this deadlock we pass the baton to a new round, as in the Broadcast-Decide algorithm, with a similar same mechanism: a switcher that times-out waiting for new messages from the acceptors broadcast a stop message, gathers the accepted history from at least one of the acceptors, and aborts. 

Let us specify more precisely the Simple-Leader algorithm.

The specification of Simple-Leader algorithm uses the same variables as the Broadcast-Decide algorithm and the same initial state. 
%that is the arrays of variables    
%$inits\left[ s \right]$, $aborts\left[ s \right]$, $learned\left[ l \right]$,
%$status\left[ a \right]$, and $accepted\left[ a \right]$, where $l$ is a learner, $a$ is
%an acceptor, and $s$ is a switcher.

%Initially, the variables $inits$, $aborts$, and, for every
%learner $l$, $learned\left[ l \right]$ are empty sets. For every
%acceptor $a$, $status\left[ a \right]$ is ``idle'' and $hist\left[ a
%\right]$ is the special value $None$.

The next-state relation is the disjunction of the following types of
transitions. 
\begin{compactitem}
\item A $Propose\left( p,c \right)$ transition, for a proposer $p$
  and a command $c$, not guarded, sends the message $\aseq{\quo{prop},c}$ to
  the leader.
\item An $Init\left( s, h \right)$ transition, for a switcher $s$
  and a history $h$, is enabled when $inits\left[ s \right] = \quo{none}$, and its effect is to set $inits\left[ s \right]$ to $h$ and
  to sends the message $\aseq{\quo{init},h}$ to the leader.
\item A $WakeUp\left( leader \right)$ transition, enabled when
  $status\left[ leader \right]$ is ``idle'' and 
  a message $\aseq{\quo{init},h}$ was sent to the leader. Its effect is to
  receive the message, set $accepted\left[ leader \right]$ to $h$,
  to set $status\left[ leader \right]$ to ``ready'', and to
  broadcast the message $\aseq{\quo{leader-accept},h}$ to all
  other acceptors.
\item An $Accept\left( leader \right)$ transition, enabled when
  $status\left[ leader \right]$ is ``ready'' and a $\aseq{\quo{prop},c}$ message has been sent to the leader. The effect of the
  action is to receive the message, to update $accepted\left[ leader
  \right]$ to $accepted\left[ leader \right]\star c$, and to
  broadcast the message $\aseq{\quo{accepted},accepted'\left[ leader
  \right]}$ to the learners, and to broadcast the message
  $\aseq{\quo{leader-accept},accepted'\left[ leader \right]}$
  to all the other acceptors.
\item An $Accept\left( a \right)$ transition, for an acceptor $a$
  which is not the leader, is enabled when $status\left[ a
  \right]$ is ``ready'' and the leader has sent a
  $\aseq{\quo{leader-accept},h}$ message to $a$, where $h > accepted\left[ a
  \right]$. The effect of the action is to receive the message,
  to update $accepted\left[ a \right]$ to $h$, and to broadcast the
  message $\aseq{\quo{accepted},hist\left[ a \right]\star c}$ to the
  learners.
\item A $Learn\left( l, h \right)$ transition, for a learner $l$
  and a history $h$, is enabled when $h \geq learned\left[ l \right]$ and $h$ is a lower
  bound of the set $\aset{MaxReceivedFrom\left( a \right) : a \in Acceptors}$, where $MaxReceivedFrom\left( a \right)$ is the maximal accepted history received
  from the acceptor $a$.  Its effect is to set $learned\left[ l \right]$ to $h$.
\item A $Stop\left( s \right)$ transition, for a switcher $s$,
  enabled when the switcher $s$ has waited for too long without receiving any messages from the acceptors, broadcasts a $\aseq{\quo{stop}, s}$ message to all
  the acceptors.  
\item A $Stop\left( a \right)$ transition, for an acceptor $a$,
  enabled when $a$ can receive a $\aseq{\quo{stop},s}$ message
  from a switcher $s$. Its effect is to receive the message, set
  $status\left[ a \right]$ to ``stopped'', and to send the
  message $\aseq{\quo{stopped},a,hist\left[ a \right]}$ to the
  switcher $s$.
\item An $Abort\left( s, h \right)$ transition, for a switcher $s$ and a history $h$, is enabled when $aborts\left[ s \right] = \quo{none}$ and
  the switcher has received at least one $\aseq{\quo{stopped}, a, h}$ message. Its effect is to set the variable $aborts\left[ s \right]$ to the
  history $h$ received in the $\quo{stopped}$ message.
\end{compactitem}

Thanks to the inter-round interface, which is the same for Simple-Leader rounds and Broadcast-Decide rounds, Simple-Leader rounds and Broadcast-Decide rounds can be repeatedly composed in a sequence. 

Informally, the composition is correct because abort histories are always safe in the sense that any of the abort histories could be the next learned history.
However not all can be learned and the initialization mechanism of the rounds ensures that at most one is chosen.
In the next section we present Composable Generalized Consensus, a specification of the behavior of rounds at the level of proposals, learned histories, init histories, and abort histories. Our composition theorem ensures that any set of rounds individually satisfying Composable Generalized Consensus can be composed in any order to yield an algorithm satisfying Composable Generalized Consensus. 

\section{Composable Generalized Consensus}
\label{sec:aca}

We now consider implementing Generalized Consensus as a sequence of \emph{Composable Generalized Consensus rounds}, where each round makes progress
ordering requests or aborts its execution and passes the baton to the next round in the sequence. An algorithm composed of a sequence of CGC instances
is called a \emph{CGC algorithm}.

On top of the traditional roles of proposer, acceptor, and learner, CGC
introduces the new role of \emph{switcher} and a new component, the
\emph{scheduling policy}. In CGC, the assignment of the roles of proposer,
acceptor, learner, and switcher can change in each rounds, allowing for
reconfiguration. 
In normal circumstances, the proposers, acceptors,
and learners of a CGC instance work together to implement Generalized Consensus.
However, if the environment becomes unfavorable to a running CGC instance, the
scheduling policy can instruct the round to abort its execution and pass the
baton, using the switchers, to a new round.

A switcher of an aborting round determines, from the state of the
acceptors, an \emph{abort sequence} that it transfers to one or several
switchers of the new round.  Upon receiving an abort sequence, a switcher of the
new round initializes the acceptors of the new round, which can then
proceed implementing SMR, until the new round itself aborts and the
process is repeated. 
We assume that in the first round, which has no previous round, every 
switchers receives the empty abort sequence.

The scheduling policy determines when to abort a CGC instance and which new CGC
round should take over. The scheduling policy must inform the switchers about the
identity of the next round and must guarantee that all switchers receive the
same information.  In this paper we abstract over the scheduling policy, assuming
that a round that aborts passes the baton to a nondeterministically chosen new
round. A reliable scheduling-policy component can realistically be implemented
as a replicated state machine running on a large number of servers otherwise
dedicated to other tasks
\cite{LamportMalkhiZhou09VerticalPaxosPrimarybackupReplication}. We will see
 in \cref{sec:rounds} that, as in Vertical Paxos, a reliable scheduling policy component allows
tolerating $f$ faults with $f+1$ replicas, while maintaining the long-term
resilience of the service by replacing faulty servers upon round change.

The clients of a CGC algorithm can send their commands to the proposers of any round (in
practice, clients send their commands to proposers of the largest
active round) and servers can use sequences learned in any round
to determine outputs.

CGC specifies the allowed behaviors of the proposers, learners, and switchers
\emph{of a single round}. Similarly to
Generalized Consensus, we use the state variables $crashed$, $proposed$,
and, for every learner $l$, $learned\left[ l \right]$.  Additionally,
we use the state variables $inits$, containing the abort sequences
received from the previous round, and $aborts$, containing the abort
sequences transfered to the next round.
Two consecutive CGC instances are composed by letting the $aborts$ variable of the
first round be the $inits$ variable of the second round.

Initially, $crashed$, $proposed$, for every
learner $l$, $learned\left[ l \right]$, $inits$, and $aborts$ are all empty sets.
The next-state relation consists of five types of transitions. The
first three, $Crash\left( x \right)$, $Propose\left( l, c
\right)$, and $Learn\left( l, cs \right)$, for a learner, proposer, or
switcher $x$, for a learner $l$, and for a sequence of commands
$cs$, have the same effect as in Generalized Consensus.
CGC introduces two new transitions, $Init\left(
s, cs \right)$ and $Abort\left( s, cs \right)$, for a switcher $s$ and a sequence of
commands $cs$. The transition $Init\left( s, cs \right)$ adds $cs$ to the set $inits$, and models a switcher receiving the
abort sequence $cs$ from the preceding round.
The transition $Abort\left( s, cs \right)$ adds $cs$ to the set $aborts$ and models a switcher transferring the abort sequence $cs$ to a switcher of the next round.
All the transition are guarded by the following conditions:
\begin{compactitem}
    \item[\textbf{Agreement}:] The set $\aset{learned'\left[ l
      \right] : l \in Learners}$ is compatible.
    \item[\textbf{Validity}:] There is a history $h$ belonging to $inits$ such that every abort history or learned history 
        is of the form $h \star cs$, where $cs$ contains only commands
        of the set $proposed$.
    \item[\textbf{Initialization}:] If $inits$ is empty, then no
        sequence can be learned and no switcher can abort.
    \item[\textbf{Irrevocability}:] In the case of a $Learn\left( l,h
        \right)$ transition, the new learned history $h$ is an 
        extension of $learned\left[ l \right]$.
    \item[\textbf{Safe Abort}:] Every abort sequence is an extension in the 
        partial order $\leq$ of a learned history.
\end{compactitem}
Note that Agreement and Irrevocability are the same as in Generalized Consensus.

If a client has submitted a command to a proposer, eventually,
either the client gets a response from the current round, or
the next round is initialized with an abort sequence and the
command can be resubmitted to the next round.

\subsection{The Composition Theorem}

TODO: frame in terms of composability.

Consider the succession of $n$ rounds numbered $1$ to $n$.  To
distinguish the variables of each round, we annotate the local
variables of each round with the number of the round. For example, the
variables of the seventh round are written $proposed_7$, for every
learner $l$, $learned_7\left[ l \right]$, $inits_7$, and $aborts_7$.

Moreover, we assume that a command, which is proposed to a round and that
is not learned before the first abort sequence is transfered to the
next round, is proposed anew to the next round.

Define the state variable $proposed$ as the union over all rounds $i$
of the $proposed_i$ variables and, for every learner $l$, 
define the $learned\left[ l \right]$ variable as the union over all
rounds $i$ of the $learned_i\left[ l \right]$ variables.

\begin{thm}[Composition Theorem]
    \label{thm:comp}
    In every execution of a succession of CGC instances, the sequence of valuations
    of the variable $proposed$ and, for every learner $l$, of the variables
    $learned\left[ l \right]$ satisfies Generalized Consensus.
\end{thm}
\Cref{thm:comp} can be shown by induction on the number of rounds,
using the following lemma.
\begin{lem}
  \label{lem:comp}
    Consider two rounds numbered $i$ and $i+1$. In every execution of a
    succession of CGC instances, the sequence of valuations of the variables $inits
    = inits_i$, $propose = proposed_i \cup propose_{i+1}$, for every learner
    $l$, $learned\left[ l \right] = learned_i\left[ l \right] \cup
    learned_{i+1}\left[ l \right]$and $aborts = aborts_{i+1}$ satisfies CGC.
\end{lem}
\begin{proof}
  A mechanically-checked proof in Isabelle/HOL appears in \cref{sec:isaproofs}.
  The proofs uses a version of the specification of CGC that has a more
  executable form that presented in this section. Moreover,
  instead of sequences of commands, the Isabelle/HOL specification uses the more
  general notion of command history, presented in the next subsection.
  The Isabelle/HOL formalization of CGC uses the theory of I/O automata, which
  is also formalized in the Isabelle/HOL. The proof uses a refinement
  mapping from the composition of two consecutive CGC instances to a single CGC
  instance.
\end{proof}

\Cref{thm:comp} guarantees that the successive composition of any
number of CGC instances implements Generalized Consensus.
Therefore, with CGC, one can easily build replicated services that
dynamically adapt to the current operating conditions of the system: a
replicated service has several types of rounds at its disposal, each being
tailored to particular conditions. Each time the conditions change,
the current CGC instance aborts and passes the baton to a more 
appropriate type of round, as determined by the scheduling policy. Moreover, a new type
of CGC instance can always be devised after the system is deployed and added on the fly to
the existing types of rounds.

We now describe two important practical optimizations of CGC.

\subsection{Optimizing the Execution of Read-Only Commands}
\label{sec:readonly}

Generalized Consensus can be further optimized by treating 
read-only commands specially, as in Generalized Lattice Agreement
\cite{FalerioETAL12GeneralizedLatticeAgreement}.

We consider the case in which a set of servers each play the four
roles of proposer, acceptor, learner, and switcher.
Clients sends their commands to any server, which proposes them as a
proposer, and records the command as pending.
%When a server learns a history that contains one of its pending
%commands, it uses the history to determine the response to the pending
%command and forwards the response to the appropriate client.  

As observed in \cite{FalerioETAL12GeneralizedLatticeAgreement}, read-only
commands can be executed faster by directly reading from the last learned
history, instead of proposing the read commands and waiting for it to appear in
a future learned history.  Because learned histories form a chain, all the reads
return consistent values and this approach yields a \emph{serializable}
implementation of the service.

However linearizability is not satisfied if reads are served directly
from the last learned history: a lagging replica server could respond
to a read using a history which does not contain all completed update
commands. To make the executions linearizable without resorting to
proposing read-only commands as normal commands, we can employ the same
technique as in \cite{FalerioETAL12GeneralizedLatticeAgreement}: upon
receiving a read command, a server proposes a special no-op command
that commutes with all other commands; the server then responds to the
read command using the first learned history containing the no-op
command. This ensures that all the commands that had completed before
the read command was issued are present in the history.  Moreover,
since no-op commands commute with all other commands, they can be
treated more efficiently than regular read commands by the acceptors,
like, for example, in a fast round of Fast Paxos.

\section{Fast and Conservative Rounds}
\label{sec:rounds}

Most existing SMR algorithms can easily be obtained as the
composition of one or more CGC instances. 

The FLP impossibility result \cite{FischerLynchPaterson83ImpossibilityDistributedConsensusOneFaultyProcess}
implies that no algorithm can implement SMR to the letter in practice.
Instead, SMR algorithms guarantee safety (i.e. replica execute the
same sequence of commands) but may not be able to make progress (i.e.
order new commands). To make progress as likely as possible, most SMR
algorithms are structured in consecutive rounds (also called ballots,
views, or epochs). In each round, the algorithm tries to make progress
but cannot be guaranteed to succeed. When the algorithm detects that
progress is no more possible, a new round is started.  The new round
is initialized so as to preserve safety, usually by one of the
acceptors. For example, in Paxos, the leader of a newly started ballot
numbered $i$ determines a value that is \emph{safe at $i$} and
proposes it as first value to the other acceptors of the ballot $i$.
In existing SMR algorithms, new rounds are initialized ad-hoc, which
prevents mixing rounds of two different algorithms.  Instead,
CGC is an abstraction of rounds which establishes a common interface and contract for switching from
one round to the next, enabling switching from any CGC instance to any other.

To simplify the task of expressing existing algorithms as the composition of one
or more CGC instances, we propose two specifications, \emph{conservative CGC
rounds} and
\emph{fast CGC instances}, which can easily be concretized to obtain executable rounds.
We show that CGC versions of Classic Paxos, Multi-Coordinated Paxos, Chain
Replication, and Ring Paxos can be obtained by concretizing the specification of
conservative CGC instances, and that CGC versions of the fast rounds of Fast Paxos can be obtained by concretizing the specification of fast CGC instances.

Fast and conservative CGC instances are two specifications, at an
intermediate level of abstraction, which satisfy CGC. Both
specifications specify how the state of the acceptors should be
updated but abstract over the particular communication topology used
to implement the state accesses: the specifications use a global state
which is accessible by all servers in all roles. One can refine fast
or conservative rounds by implementing the state accesses and coordination
using the network, obtaining a concrete algorithm.

Conservative and fast CGC instances are specified using the state variables $inits$, $proposed$, for every learner $l$, $learned\left[ l \right]$
and $aborts$, as in CGC, and, for every acceptor $a$, two new variables
$status\left[ a \right]$, which can be either ``idle'', ``ready'', or
``stopped'' and $hist\left[ a \right]$, the local history of the
acceptor $a$, which can be either a history or the special value $None$.
Initially, $inits$, $aborts$, and, for every learner
$l$, $learned\left[ l \right]$ are empty sets, and, for every acceptor $a$,
$status\left[ a \right]$  is ``idle'' and $hist\left[ a \right]$ is the special
value $None$.

The next-state relations of conservative and fast rounds are composed
of the following types of transitions (and the default $Crash\left( x
\right)$ transition).
\begin{compactitem}
    \item A $Propose\left( p,c \right)$ transition, for a proposer $p$
        and a command $c$, not guarded, inserts $c$ in the set
        $proposed$. 
    \item An $Init\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, not guarded, inserts $h$ in the set $inits$.
    \item A $WakeUp\left( a \right)$ transition, for an acceptor $a$,
        is enabled when $status\left[ a \right]$ is ``idle''. Its
        effect is to set $hist\left[ a \right]$ to a history found in
        $inits$ and to set $status\left[ a \right]$ to ``ready''.
    \item An $Accept\left( a \right)$ transition, for an acceptor $a$,
        is enabled when $status\left[ a \right]$ is ``ready'', and
        updates $hist\left[ a \right]$ to $hist\left[ a \right]\star
        c$, where $c$ is a proposed command. In conservative rounds,
        the transition is additionally guarded by the condition that
        the resulting set of local acceptor histories,
        $\aset{hist'\left[ a \right] : a \in Acceptors}$, forms a chain
        in the partial order $\leq$ (this is typically implemented with a
        leader).
    \item A $Learn\left( l, h \right)$ transition, for a learner $l$
      and a history $h$, enabled when $h > learned\left[ l \right]$ and is
      smaller than the GLB of the histories of a learn quorum of acceptors. The effect of
        the transition is to set $learned\left[ l \right]$ to $h$.
    \item A $Stop\left( a \right)$ transition, for an acceptor $a$,
        enabled when $status\left[ a \right]$ is ``ready'', sets
        $status\left[ a \right]$ to ``stopped'' (note that
        $status\left[ a \right] = $ ``stopped'' prevents any further
        ``accept'' transition of acceptor $a$).
%    \item REALLY NEEDED? A $TimeOut\left( a \right)$ transition, for an acceptor $a$,
%        is enabled when $status\left[ a \right]$ is ``idle'' and sets 
%        $status\left[ a \right]$ to ``stopped'' and $hist\left[ a
%        \right]$ to the special value $None$. 
    \item An $Abort\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, is enabled when there exists a abort quorum
        $R$ such that, for every member $a$ of $R$, $status\left[ a
        \right]$ is ``stopped'' and $h \in SafeAborts\left( R
        \right)$. Its effect is to insert  $h$ in the set $aborts$. 
\end{compactitem}

On top of the additional guard on the $Accept$ transition present only
in conservative rounds, conservative and fast CGC instances differ in
their definition of learn quorums, abort quorums, and in their
definition of the $SafeAborts\left( R \right)$ operator.
To concisely define read and learn quorums, we define 
\begin{equation}
    Q\left( f \right) = \aset{as \subseteq Acceptors :
        Card\left(as\right) \geq \left\lfloor f\times Card\left(
    Acceptors \right)\right\rfloor + 1}.
\end{equation}

In conservative CGC instances, abort and learn quorums are defined such
that the intersection between two learn quorums is nonempty and the
intersection between a learn quorum and an abort quorum is nonempty.
The definition leads to two interesting cases: $LearnQ = AbortQ =
Q\left( 1/2 \right)$; $LearnQ = \aset{Acceptors}$ and $AbortQ =
\aset{\aset{a} : a \in Acceptors}$. The operator $SafeAborts\left( R
\right)$ is defined as the singleton set containing the maximum
element over the local histories of the acceptors in $R$ which are not
$None$. Note that the maximum exists because conservative rounds
guarantee that the histories of the acceptors form a chain.

In fast CGC instances, abort and learn quorums are defined such that
the intersection between any two learn quorums is nonempty and the
intersection between any two learn quorums and an abort quorum is
nonempty.  The definition leads to three interesting cases: $LearnQ =
AbortQ = Q\left( 2/3 \right)$; $LearnQ = Q\left( 1/2 \right)$ and
$AbortQ = Q\left( 3/4 \right)$; $LearnQ = \aset{Acceptors}$ and
$AbortQ = \aset{\aset{a : a \in Acceptors}}$.
The operator $SafeAborts\left( R \right)$ is defined as the set of
maximal elements over the set $\aset{GLB\left( \aset{hist\left[ a
\right] : a \in R \cap L \wedge hist\left[ a \right]\neq None} \right)
: L \in LearnQ}$.

The cases in which abort quorums are singleton sets, in both
conservative and fast rounds, allow tolerating
$f$ faults with $f+1$ servers: a round can abort as long as one
acceptor is correct and a new round, possibly using new correct
servers, can take over. Note that changing round cannot happen
without the scheduling policy instructing the switchers about the
identity of the next round, and thus tolerating $f$ faults with
$f+1$ servers relies on the assumption of a reliable scheduling policy.

Define, for every learner $l$, the history variable $\overline{learned}\left[ l \right]$ which is initialized to the empty set and which is
updated each time a new local history is accepted by the learner $l$ by
inserting the new local history in $\overline{learned}\left[ l \right]$ (see \cite{AbadiLamport91ExistenceRefinementMappings} for the
definition of a history variable).
\begin{thm}
  \label{thm:rounds}
    Both conservative and fast CGC instances implement CGC under the refinement
    mapping that consists in substituting $\overline{learned\left[ l \right]}$
    for $learned\left[ l \right]$.
\end{thm}


\begin{comment}
\section{Examples of Concrete CGC Algorithms}
\label{sec:examples}

In this section we show how to modify the rounds of Classic Paxos,
Multi-Coordinated Paxos, Chain Replication, Ring Paxos, and Fast Paxos to obtain
CGC instances. \Cref{thm:comp} then guarantee that all those different types of
rounds can be used in the same algorithm to yield an implementation of Chain
Agreement.

\subsection{Classic Paxos}

The rounds of the Classic Paxos algorithm can be modified so as to
obtain an implementation of conservative CGC instances.  We define
Classic Paxos CGC instances by specializing the specification of
conservative CGC instances, giving a concrete protocol for disseminating
the proposed commands to the acceptors and enforcing that the learned
histories form a chain.

Each Classic Paxos round has a unique leader among the acceptors which
centrally orders the proposed commands, which are accepted by the
other acceptors in the order determined by the leader.  This protocol
ensures that every local acceptor history is a prefix of the local
history of the leader, therefore guaranteeing that the local acceptor histories
forming a chain.

Acceptor broadcast each newly accepted history to the learners, which
learn a history $h$ when they receive enough extensions of $h$ from the
acceptors.  When instructed by the scheduling policy to abort, a
switcher sends a stop message to the acceptors, which irrevocably stop
updating their local history and send back to the switcher an
acknowledgement containing their local history. A switcher
determines an abort history once it has received acknowledgments from
an abort quorum of acceptors $R$, using the operator $SafeAborts\left(
R \right)$.

The specification of Classic Paxos CGC instances uses the variables of
the specification of conservative CGC instances, namely the variables
$proposed$, $inits$, $aborts$, the arrays $learned\left[ l \right]$,
$status\left[ a \right]$, and $hist\left[ a \right]$, and two
additional arrays $accepted\left[ l \right]\left[ a \right]$ and
$stop\left[ s \right]\left[ a \right]$, where $l$ is a learner, $a$ is
an acceptor, and $s$ is a switcher.

There is also a network component that we do not describe explicitly.
However, the network allows any servers to communicate by sending and
receiving messages.

Initially, the variables $proposed$, $inits$, $aborts$, and, for every
learner $l$, $learned\left[ l \right]$ are empty sets. For every
acceptor $a$, $status\left[ a \right]$ is ``idle'' and $hist\left[ a
\right]$ is the special value $None$. For every learner $l$ and acceptor $a$,
$accepted\left[ l \right]\left[ a \right]$ is the special value
$None$. For every switcher $s$ and acceptor $a$, $stopped\left[ s
\right]\left[ a \right]$ is set to $false$.  There are initially no
messages in the network.

The next-state relation is composed of the following types of
transitions, where the definitions of learn quorums, abort quorums,
and of the $SafeAborts\left( R \right)$ operator are the ones of conservative
CGC instances.
\begin{compactitem}
    \item A $Propose\left( p,c \right)$ transition, for a proposer $p$
        and a command $c$, not guarded, inserts $c$ in the set
        $proposed$ and sends the message $\aseq{\quo{prop},c}$ to
        the leader.
    \item An $Init\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, not guarded, inserts $h$ in the set $inits$ and
        sends the message $\aseq{\quo{init},h}$ to the leader.
    \item A $WakeUp\left( leader \right)$ transition, enabled when
        $status\left[ leader \right]$ is ``idle'' and the leader can
        receive a $\aseq{\quo{init},h}$ message. Its effect is to
        receive the message, set $hist\left[ leader \right]$ to $h$,
        to set $status\left[ leader \right]$ to ``ready'', and to
        broadcast the message $\aseq{\quo{leader-accept},h}$ to all
        other acceptors.
    \item An $Accept\left( leader \right)$ transition, enabled when
        $status\left[ leader \right]$ is ``ready'' and the leader can
        receive a $\aseq{\quo{prop},c}$ message. The effect of the
        action is to receive the message, to update $hist\left[ leader
        \right]$ to $hist\left[ leader \right]\star c$, and to
        broadcast the message $\aseq{\quo{ack},hist\left[ leader
        \right]\star c}$ to the learners, and to broadcast the message
        $\aseq{\quo{leader-accept},hist\left[ leader \right]\star c}$
        to all the other acceptors.
    \item An $Accept\left( a \right)$ transition, for an acceptor $a$
        which is not the leader, is enabled when $status\left[ a
        \right]$ is ``ready'' and $a$ can receive a
        $\aseq{\quo{leader-accept},h}$ message where $h > hist\left[ a
        \right]$. The effect of the action is to receive the message,
        to update $hist\left[ a \right]$ to $h$, and to broadcast the
        message $\aseq{\quo{ack},hist\left[ a \right]\star c}$ to the
        learners.
    \item A $RcvAck\left( l \right)$ transition, for a learner $l$, is
        enabled when $l$ can receive a message $\aseq{\quo{ack},h}$
        from an acceptor $a$. Its effect is to receive the message and
        to set $accepted\left[ l \right]\left[ a \right]$ to $h$.
    \item A $Learn\left( l, h \right)$ transition, for a learner $l$
        and a history $h$, is enabled when $h > Max\left(
        learned\left[ l \right] \right)$ and there is a learn quorum
        $Q$ of acceptors such that $h = GLB\left( \aset{accepted\left[
        l \right]\left[ a \right] : a \in Q }\right)$. Its effect is
        to insert $h$ in $learned\left[ l \right]$.
    \item A $Stop\left( s \right)$ transition, for a switcher $s$,
        always enabled, broadcasts a $\aseq{\quo{stop}}$ message to all
        the acceptors.  This transition models a switcher being
        instructed by the scheduling policy to abort the current round.
    \item A $Stop\left( a \right)$ transition, for an acceptor $a$,
        enabled when $a$ can receive a $\aseq{\quo{stop}}$ message
        from a switcher $s$. Its effect is to receive the message, set
        $status\left[ a \right]$ to ``stopped'', and to send the
        message $\aseq{\quo{stopped},hist\left[ a \right]}$ to the
        switcher $s$.
    \item A $RcvStopped\left( s \right)$ transition, for a switcher
        $s$, enabled when $s$ can receive a $\aseq{\quo{stopped},h}$
        message from an acceptor $a$. Its effect is to receive the
        message and to set $stopped\left[ s \right]\left[ a \right]$
        to $h$.
    \item An $Abort\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, is enabled when there exists an abort quorum
        $R$ such that, for every member $a$ of $R$, $stopped\left[ s
        \right]\left[ a \right]$ is not $false$ and $h \in
        SafeAborts\left( R \right)$, where $SafeAborts\left( R
        \right)$ is computed using the array $stopped\left[ a
        \right]$. Its effect is to insert  $h$ in the set $aborts$. 
\end{compactitem}
Compared to the rounds of the original Classic Paxos, Classic Paxos CGC
rounds add an extra level of indirection when changing round: in the
original rounds, the leader of the new round directly queries the
acceptors of the preceeding round, whereas in CGC instances, the
switchers query the acceptors and then send their abort histories to
the leader. However, changing round is an infrequent event, therefore
this difference should not have much practical impact on performance.
\begin{thm}
  Classic Paxos CGC instances satisfy Composable Generalized Consensus.
\end{thm}
\begin{proof}
  A Classic Paxos CGC instance implements the specification of Abortable
  Generalized Consensus under the refinement mapping consisting in
  projecting the state of the round onto the variables $proposed$,
  $inits$, $aborts$, and, for every learner $l$, $learned\left[ l
  \right]$.
\end{proof}

\end{comment}
\subsection{Multi-Coordinated Paxos, Chain Replication, and Ring Paxos.}

Multi-Coordinated Paxos, Chain Replication, and Ring Paxos rounds have a similar
structure as Classic Paxos rounds but differ in the way the leader is
implemented and in the way that the histories of the leader are disseminated to
the other acceptors.

Multi-Coordinated Paxos avoids depending on a unique leader by using a
distributed leader implementation that can make progress as long as a
strict majority of the acceptors are correct, trading off resilience
for a higher time complexity. Conservative CGC
rounds can be implemented like with Classic Paxos, replacing the
leader by the distributed leader of Multi-Coordinated Paxos.

In Chain Replication, acceptors are arranged in a chain whose head is
the leader of the round. As in Classic Paxos, abort histories and
proposed commands are sent to the leader. However, instead of the
leader directly sending its local history to all the other acceptors,
the local history of the leader is
forwarded along the chain from one acceptor to the other until it
reaches the last acceptor, which sends the history to the learners. 
By spreading the load more evenly on the all the servers, Chain Replication
can achieve higher throughput that Classic Paxos.
Conservative CGC instances can be implemented like with Classic Paxos,
replacing the broadcast of the leader by the propagation of the leader's history
along the chain of servers.

Ring Paxos optimizes Chain Replication by having the leader broadcast
commands to the acceptors using IP multicast and only sending hashes
of the commands along the chain. Ring Paxos rounds can be modified to implement
conservative CGC instances in a similar way as for Chain Replication.

\begin{comment}
\subsection{Fast Paxos}

Fast Paxos is composed of two types of rounds: classic rounds, like in Classic
Paxos, and fast rounds.
The fast rounds can be transformed into
implementations of fast CGC instances. Instead of sending their
commands to a leader, as in Classic Paxos, the proposers of fast
rounds directly broadcast their commands to all the acceptors. As long
as commands are received in the same order by a learn quorum of
acceptors or are received in different orders but commute, commands can be learned in two message-propagation delays instead of three in Classic Paxos.
However, if non-commuting commands are received in different orders by
several acceptors, the local history of the acceptors may not form a
chain anymore and the rounds must abort. The fast rounds of Fast
Paxos implement fast CGC instances.

Fast Paxos CGC instances are specified using the same set of variables as for
Classic Paxos, initialized to the same values.
The next-state relation is composed of the following types of
transitions, where the definitions of learn quorums, abort quorums,
and of the $SafeAborts\left( R \right)$ operator are the ones of fast
CGC instances.
\begin{compactitem}
    \item A $Propose\left( p,c \right)$ transition, for a proposer $p$
        and a command $c$, not guarded, inserts $c$ in the set
        $proposed$ and broadcasts the message $\aseq{\quo{prop},c}$ to
        all the acceptors.
    \item An $Init\left( s, h \right)$ transition, for a switcher $s$
        and a history $h$, not guarded, inserts $h$ in the set $inits$ and
        broadcasts the message $\aseq{\quo{init},h}$ to all the acceptors.
    \item A $WakeUp\left( a \right)$ transition, for an acceptor $a$,
        is enabled when $status\left[ a \right]$ is ``idle'' and $a$
        can receive an $\aseq{\quo{init},h}$ message. Its effect is to
        receive the message, set $hist\left[ a \right]$ to $h$, and to
        set $status\left[ a \right]$ to ``ready''.
    \item An $Accept\left( a \right)$ transition, for an acceptor $a$,
        is enabled when $status\left[ a \right]$ is ``ready'' and $a$
        can receive a $\aseq{\quo{prop},c}$ message. The effect of the
        action is to receive the message, to update $hist\left[ a
        \right]$ to $hist\left[ a \right]\star c$, and to broadcast
        the message $\aseq{\quo{ack},hist\left[ a \right]\star c}$ to
        the learners.
    \item The $RcvAck\left( l \right)$ transitions, for a learner $l$,
        the $Learn\left( l, h \right)$ transitions, for a learner $l$
        and a history $h$, the $Stop\left( s \right)$ transitions, for
        a switcher $s$, $Stop\left( a \right)$ transitions, for an
        acceptor $a$, the $RcvStopped\left( s \right)$ transitions,
        for a switcher $s$, and the $Abort\left( s, h \right)$
        transitions, for a switcher $s$ and a history $h$, which are
        all the same as in Classic Paxos CGC instances.
\end{compactitem}

As for Classic Paxos CGC instances, Fast Paxos CGC instances implement CGC
under the refinement mapping consisting projecting the state of the
round onto the variables $proposed$, $inits$, $aborts$, and, for every
learner $l$, $learned\left[ l \right]$.


\end{comment}

\newpage

\printbibliography


\begin{appendix}
  
  \label{sec:isaproofs}
 
%  \includepdf[pages={1},pagecommand={\section{Mechanical Proof of the
%  Composition Theorem in Isabelle/HOL}\label{sec:isaproofs}},scale=1]{../IsabellePDFS/podctheories.pdf}
%  \includepdf[pages={2-},pagecommand={},scale=1]{../IsabellePDFS/podctheories.pdf}
  
\end{appendix}

\end{document}
